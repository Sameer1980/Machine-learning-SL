from pyspark.sql.types import StringType,DoubleType,IntegerType


df.filter(df['mobile']=='Vivo').show()



from pyspark.sql.types import StringType,DoubleType,IntegerType


df.filter(df['mobile']=='Vivo').show()

df.filter((df['_c4']=='Vivo')&(df['_c2'] >10)).show()

# Value counts
df.groupBy('mobile').count().show(5,False)

# Value counts
df.groupBy('mobile').count().orderBy('count',ascending=False).show(5,False)


#Aggregationdf.groupBy('mobile').agg({'experience':'sum'}).show(5,False)


# Value countsdf.groupBy('mobile').min().show(5,False)

df=df.dropDuplicates()


df = spark.createDataFrame([ (1, 143.5, 5.6, 28,   'M',  100000),(2, 167.2, 5.4, 45,   'M',  None),(3, None , 5.2, None, None, None),(1, 144.5, 5.9, 33, 'M'),        (2, 167.2, 5.4, 45, 'M'),        (3, 124.1, 5.2, 23, 'F'),        (4, 144.5, 5.9, 33, 'M'),        (5, 133.2, 5.7, 54, 'F'),        (3, 124.1, 5.2, 23, 'F'),        (5, 129.2, 5.3, 42, 'M'),], ['id', 'weight', 'height', 'age', 'gender'])



print('Count of rows: {0}'.format(df.count()))

print('Count of distinct rows: {0}'.format(df.distinct().count()))

df = df.dropDuplicates()

df.show()


df_miss.rdd.map(    lambda row: (row['id'], sum([c == None for c in row]))).collect()

df_miss.rdd.map(    lambda row: (row['id'], sum([c == None for c in row]))
print('Count of rows: {0}'.format(df.count()))print('Count of distinct rows: {0}'.format(df.distinct().count()))




from pyspark.ml.linalg import Vectorsfrom pyspark.ml.stat 
import Correlation
data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),), 
      (Vectors.dense([4.0, 5.0, 0.0, 3.0]),), 
      (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),     
      (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]df = spark.createDataFrame(data, ["features"])r1 = Correlation.corr(df, "features").head()
print("Pearson correlation matrix:\n" + str(r1[0]))
r2 = Correlation.corr(df, "features", "spearman").head()
print("Spearman correlation matrix:\n" + str(r2[0]))