{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train_ds = pd.read_csv( \"sentiment_train\", delimiter=\"\\t\" )\n",
    "train_ds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\n",
    "df_model_selection = pd.DataFrame(columns=COLUMN_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Process, Model Name, F1 Scores, Range of F1 Scores, Std Deviation of F1 Scores]\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y):\n",
    "    global df_model_selection\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=29)\n",
    "    weighted_f1_score = []\n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[val_index] \n",
    "        y_train, y_test = y[train_index], y[val_index]\n",
    "        model_obj.fit(X_train, y_train)##### HERE ###\n",
    "        test_ds_predicted = model_obj.predict( X_test ) ##### HERE ####   \n",
    "        #print( metrics.classification_report( y_test, test_ds_predicted ) )    \n",
    "        weighted_f1_score.append(round(f1_score(y_test, test_ds_predicted , average='weighted'),2))\n",
    "        \n",
    "    sd_weighted_f1_score = np.std(weighted_f1_score, ddof=1)\n",
    "    range_of_f1_scores = \"{}-{}\".format(min(weighted_f1_score),max(weighted_f1_score))    \n",
    "    df_model_selection = pd.concat([df_model_selection,pd.DataFrame([[process,model_name,sorted(weighted_f1_score),range_of_f1_scores,sd_weighted_f1_score]], columns =COLUMN_NAMES) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[train_ds.sentiment == 1][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>da vinci code was a terrible movie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>Then again, the Da Vinci code is super shitty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>The Da Vinci Code comes out tomorrow, which su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>i thought the da vinci code movie was really b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>0</td>\n",
       "      <td>God, Yahoo Games has this truly-awful looking ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "3943          0                da vinci code was a terrible movie.\n",
       "3944          0  Then again, the Da Vinci code is super shitty ...\n",
       "3945          0  The Da Vinci Code comes out tomorrow, which su...\n",
       "3946          0  i thought the da vinci code movie was really b...\n",
       "3947          0  God, Yahoo Games has this truly-awful looking ..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[train_ds.sentiment == 0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6918 entries, 0 to 6917\n",
      "Data columns (total 2 columns):\n",
      "sentiment    6918 non-null int64\n",
      "text         6918 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 108.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "plt.figure( figsize=(6,5))\n",
    "\n",
    "\n",
    "# create count plot\n",
    "ax = sn.countplot(x='sentiment', data=train_ds)\n",
    "\n",
    "# annotate\n",
    "for p in ax.patches:\n",
    "    ax.annotate(p.get_height(), (p.get_x()+0.1, p.get_height()+50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference from count plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive and negative sentiments are having fairly equal proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "# Conversion of Text data to Cross-sectional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vector Model - A process of converting text to cross-sectional data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "# Create the dictionary from the corpus\n",
    "feature_vector = count_vectorizer.fit( train_ds.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features:  2132\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names\n",
    "features = feature_vector.get_feature_names()\n",
    "print( \"Total number of features: \", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '007',\n",
       " '10',\n",
       " '10pm',\n",
       " '12',\n",
       " '16',\n",
       " '17',\n",
       " '1984',\n",
       " '1st',\n",
       " '200',\n",
       " '2007',\n",
       " '286',\n",
       " '2nd',\n",
       " '30',\n",
       " '31st',\n",
       " '33',\n",
       " '3333',\n",
       " '385',\n",
       " '50',\n",
       " '517',\n",
       " '648',\n",
       " '6th',\n",
       " '700',\n",
       " '7th',\n",
       " '8230',\n",
       " '9am',\n",
       " 'aaron',\n",
       " 'able',\n",
       " 'abortion',\n",
       " 'about',\n",
       " 'abrams',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absurd',\n",
       " 'academy',\n",
       " 'acceptable',\n",
       " 'accompaniment',\n",
       " 'according',\n",
       " 'account',\n",
       " 'achieved',\n",
       " 'aching',\n",
       " 'acne',\n",
       " 'acoustic',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'adaptation',\n",
       " 'add',\n",
       " 'addition',\n",
       " 'admired',\n",
       " 'admiring',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adult',\n",
       " 'adversity',\n",
       " 'afraid',\n",
       " 'after',\n",
       " 'afterschool',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'ah',\n",
       " 'aimee',\n",
       " 'ain',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'alarm',\n",
       " 'all',\n",
       " 'allegedly',\n",
       " 'allegory',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'although',\n",
       " 'altogether',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'america',\n",
       " 'amã',\n",
       " 'an',\n",
       " 'anatomy',\n",
       " 'anax',\n",
       " 'and',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angels',\n",
       " 'angle',\n",
       " 'animated',\n",
       " 'anime',\n",
       " 'aniwae',\n",
       " 'anne',\n",
       " 'answers',\n",
       " 'anus',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apologized',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appeals',\n",
       " 'archive',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'arenas',\n",
       " 'arguments',\n",
       " 'around',\n",
       " 'arse',\n",
       " 'artemis',\n",
       " 'article',\n",
       " 'as',\n",
       " 'ashamed',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'asshole',\n",
       " 'association',\n",
       " 'astonishingly',\n",
       " 'at',\n",
       " 'ate',\n",
       " 'atrocious',\n",
       " 'attached',\n",
       " 'attempt',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'audrey',\n",
       " 'author',\n",
       " 'avatar',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awesomely',\n",
       " 'awesomeness',\n",
       " 'awesomest',\n",
       " 'awful',\n",
       " 'awkward',\n",
       " 'axes',\n",
       " 'azkaban',\n",
       " 'baby',\n",
       " 'bachelor',\n",
       " 'back',\n",
       " 'backdrop',\n",
       " 'background',\n",
       " 'backtory',\n",
       " 'backward',\n",
       " 'bad',\n",
       " 'badness',\n",
       " 'ball',\n",
       " 'balls',\n",
       " 'ballz',\n",
       " 'ban',\n",
       " 'bangs',\n",
       " 'banning',\n",
       " 'barnyard',\n",
       " 'barry',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basket',\n",
       " 'bayers',\n",
       " 'bbm',\n",
       " 'bday',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beans',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beatles',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'becoming',\n",
       " 'becuase',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'being',\n",
       " 'believably',\n",
       " 'believe',\n",
       " 'belong',\n",
       " 'benefit',\n",
       " 'bentlys',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'biased',\n",
       " 'bible',\n",
       " 'big',\n",
       " 'biggie',\n",
       " 'bikes',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bits',\n",
       " 'bitter',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'blanks',\n",
       " 'blashpemies',\n",
       " 'blasphying',\n",
       " 'bless',\n",
       " 'blog',\n",
       " 'blogbacklinksnippet',\n",
       " 'blogbacklinktitle',\n",
       " 'blonds',\n",
       " 'blood',\n",
       " 'blows',\n",
       " 'board',\n",
       " 'bobbypin',\n",
       " 'body',\n",
       " 'bogus',\n",
       " 'bolsters',\n",
       " 'bond',\n",
       " 'bonkers',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bootlegged',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'both',\n",
       " 'bought',\n",
       " 'bound',\n",
       " 'bout',\n",
       " 'boycott',\n",
       " 'boycotted',\n",
       " 'boycotting',\n",
       " 'boys',\n",
       " 'boyy',\n",
       " 'brazil',\n",
       " 'break',\n",
       " 'bridget',\n",
       " 'briefly',\n",
       " 'brigid',\n",
       " 'brilliant',\n",
       " 'bringing',\n",
       " 'bro',\n",
       " 'broke',\n",
       " 'brokeback',\n",
       " 'brooke',\n",
       " 'broom',\n",
       " 'brother',\n",
       " 'brown',\n",
       " 'btw',\n",
       " 'budget',\n",
       " 'bullshit',\n",
       " 'bunch',\n",
       " 'burbank',\n",
       " 'burnt',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'ca',\n",
       " 'cake',\n",
       " 'calif',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'can',\n",
       " 'canceled',\n",
       " 'candy',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'capote',\n",
       " 'captain',\n",
       " 'care',\n",
       " 'career',\n",
       " 'carefully',\n",
       " 'caribbean',\n",
       " 'cars',\n",
       " 'casanova',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'catch',\n",
       " 'catcher',\n",
       " 'category',\n",
       " 'causing',\n",
       " 'center',\n",
       " 'challenge',\n",
       " 'chamber',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'character',\n",
       " 'characterization',\n",
       " 'characters',\n",
       " 'cheap',\n",
       " 'cheapened',\n",
       " 'check',\n",
       " 'chessboard',\n",
       " 'chick',\n",
       " 'chicken',\n",
       " 'childishly',\n",
       " 'children',\n",
       " 'chinese',\n",
       " 'choice',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christain',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christmas',\n",
       " 'christopher',\n",
       " 'chronicles',\n",
       " 'chronological',\n",
       " 'chunnel',\n",
       " 'cinema',\n",
       " 'claiming',\n",
       " 'clarksville',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'clearly',\n",
       " 'clickfive',\n",
       " 'clips',\n",
       " 'clit',\n",
       " 'clive',\n",
       " 'cloak',\n",
       " 'close',\n",
       " 'closet',\n",
       " 'clothed',\n",
       " 'club',\n",
       " 'clubbin',\n",
       " 'cobequid',\n",
       " 'cock',\n",
       " 'cocktail',\n",
       " 'code',\n",
       " 'codes',\n",
       " 'coherent',\n",
       " 'cold',\n",
       " 'colfer',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'colony',\n",
       " 'color',\n",
       " 'coloured',\n",
       " 'colourfully',\n",
       " 'combining',\n",
       " 'combonation',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'community',\n",
       " 'compared',\n",
       " 'comparrison',\n",
       " 'comparsions',\n",
       " 'complaints',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'comprehend',\n",
       " 'conclusion',\n",
       " 'concocted',\n",
       " 'condeming',\n",
       " 'condemnation',\n",
       " 'confess',\n",
       " 'congrats',\n",
       " 'conquering',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'consumed',\n",
       " 'contains',\n",
       " 'controversy',\n",
       " 'conversation',\n",
       " 'conversations',\n",
       " 'convo',\n",
       " 'cool',\n",
       " 'copy',\n",
       " 'correct',\n",
       " 'corrupting',\n",
       " 'cos',\n",
       " 'costume',\n",
       " 'costumes',\n",
       " 'could',\n",
       " 'count',\n",
       " 'counting',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cousins',\n",
       " 'cover',\n",
       " 'cow',\n",
       " 'cowan',\n",
       " 'cowboy',\n",
       " 'cowboys',\n",
       " 'coz',\n",
       " 'crack',\n",
       " 'crafted',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'crash',\n",
       " 'craze',\n",
       " 'crazy',\n",
       " 'created',\n",
       " 'creature',\n",
       " 'creatures',\n",
       " 'credit',\n",
       " 'creed',\n",
       " 'cried',\n",
       " 'cringe',\n",
       " 'criticized',\n",
       " 'criticizers',\n",
       " 'critics',\n",
       " 'cruise',\n",
       " 'crusade',\n",
       " 'cry',\n",
       " 'crystal',\n",
       " 'cucumber',\n",
       " 'culture',\n",
       " 'cussler',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'dakota',\n",
       " 'damn',\n",
       " 'dan',\n",
       " 'dance',\n",
       " 'daniel',\n",
       " 'danielle',\n",
       " 'darkness',\n",
       " 'dart',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'dating',\n",
       " 'davinci',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'deals',\n",
       " 'dearly',\n",
       " 'death',\n",
       " 'debates',\n",
       " 'decaying',\n",
       " 'decent',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decides',\n",
       " 'deciding',\n",
       " 'decline',\n",
       " 'decomposing',\n",
       " 'dedicated',\n",
       " 'deemed',\n",
       " 'deep',\n",
       " 'defensive',\n",
       " 'definately',\n",
       " 'definitely',\n",
       " 'degraw',\n",
       " 'delayed',\n",
       " 'delicious',\n",
       " 'deluded',\n",
       " 'demeantor',\n",
       " 'dementors',\n",
       " 'demons',\n",
       " 'denial',\n",
       " 'depp',\n",
       " 'depressing',\n",
       " 'depth',\n",
       " 'derek',\n",
       " 'deserved',\n",
       " 'desperate',\n",
       " 'desperately',\n",
       " 'despise',\n",
       " 'despised',\n",
       " 'devastate',\n",
       " 'devil',\n",
       " 'diana',\n",
       " 'dick',\n",
       " 'dictate',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'didnâ',\n",
       " 'die',\n",
       " 'died',\n",
       " 'dies',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'dim',\n",
       " 'dinner',\n",
       " 'directed',\n",
       " 'director',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'discovered',\n",
       " 'discuss',\n",
       " 'discussed',\n",
       " 'discussing',\n",
       " 'dislike',\n",
       " 'disliked',\n",
       " 'disney',\n",
       " 'disruption',\n",
       " 'dissapointed',\n",
       " 'diversity',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'dogfucking',\n",
       " 'dogtown',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'done',\n",
       " 'donkey',\n",
       " 'dont',\n",
       " 'donâ',\n",
       " 'doors',\n",
       " 'dork',\n",
       " 'dorks',\n",
       " 'doubt',\n",
       " 'douche',\n",
       " 'down',\n",
       " 'draco',\n",
       " 'dragged',\n",
       " 'dragons',\n",
       " 'drain',\n",
       " 'dramatic',\n",
       " 'draw',\n",
       " 'drawing',\n",
       " 'drawn',\n",
       " 'dream',\n",
       " 'dress',\n",
       " 'dressed',\n",
       " 'drive',\n",
       " 'drove',\n",
       " 'drowining',\n",
       " 'dudeee',\n",
       " 'due',\n",
       " 'dumb',\n",
       " 'dumbass',\n",
       " 'dumbest',\n",
       " 'dumbledor',\n",
       " 'dungeons',\n",
       " 'during',\n",
       " 'durno',\n",
       " 'dvd',\n",
       " 'dvds',\n",
       " 'dynamite',\n",
       " 'earrings',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'edition',\n",
       " 'editor',\n",
       " 'education',\n",
       " 'eek',\n",
       " 'effects',\n",
       " 'effort',\n",
       " 'egg',\n",
       " 'eh',\n",
       " 'either',\n",
       " 'else',\n",
       " 'emily',\n",
       " 'emma',\n",
       " 'emo',\n",
       " 'emotes',\n",
       " 'empty',\n",
       " 'encourage',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'energy',\n",
       " 'england',\n",
       " 'english',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enjoyment',\n",
       " 'enjoys',\n",
       " 'ennis',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'entitled',\n",
       " 'eoin',\n",
       " 'equal',\n",
       " 'equally',\n",
       " 'equus',\n",
       " 'eragon',\n",
       " 'erin',\n",
       " 'erm',\n",
       " 'escapades',\n",
       " 'especially',\n",
       " 'esther',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'events',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everytime',\n",
       " 'evil',\n",
       " 'evilpinkmunky',\n",
       " 'ew',\n",
       " 'exaggeration',\n",
       " 'example',\n",
       " 'excellent',\n",
       " 'except',\n",
       " 'exception',\n",
       " 'excersizing',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'executed',\n",
       " 'exelent',\n",
       " 'exhausted',\n",
       " 'expected',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'explaination',\n",
       " 'explains',\n",
       " 'exploitation',\n",
       " 'explore',\n",
       " 'explosions',\n",
       " 'expo',\n",
       " 'exponentially',\n",
       " 'exquisite',\n",
       " 'extent',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'eyes',\n",
       " 'eyre',\n",
       " 'fabricated',\n",
       " 'fabulous',\n",
       " 'facebook',\n",
       " 'facile',\n",
       " 'facing',\n",
       " 'fact',\n",
       " 'fade',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'faked',\n",
       " 'fall',\n",
       " 'fallon',\n",
       " 'families',\n",
       " 'fan',\n",
       " 'fandom',\n",
       " 'fanfic',\n",
       " 'fanfiction',\n",
       " 'fantasy',\n",
       " 'far',\n",
       " 'fat',\n",
       " 'fault',\n",
       " 'favor',\n",
       " 'favorite',\n",
       " 'favourite',\n",
       " 'fears',\n",
       " 'feast',\n",
       " 'feathers',\n",
       " 'featured',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'felicia',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'fer',\n",
       " 'festivities',\n",
       " 'few',\n",
       " 'fiber',\n",
       " 'fic',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'figured',\n",
       " 'figures',\n",
       " 'fill',\n",
       " 'film',\n",
       " 'films',\n",
       " 'final',\n",
       " 'finale',\n",
       " 'finally',\n",
       " 'finals',\n",
       " 'find',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'finshed',\n",
       " 'fire',\n",
       " 'fireworks',\n",
       " 'first',\n",
       " 'firstly',\n",
       " 'fit',\n",
       " 'fits',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'flat',\n",
       " 'flick',\n",
       " 'folk',\n",
       " 'following',\n",
       " 'folows',\n",
       " 'food',\n",
       " 'for',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forgotten',\n",
       " 'form',\n",
       " 'formed',\n",
       " 'fortress',\n",
       " 'found',\n",
       " 'four',\n",
       " 'fowl',\n",
       " 'fr',\n",
       " 'frakking',\n",
       " 'france',\n",
       " 'franchise',\n",
       " 'freagin',\n",
       " 'freak',\n",
       " 'freakin',\n",
       " 'freaking',\n",
       " 'free',\n",
       " 'freezing',\n",
       " 'frenzied',\n",
       " 'freshman',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'friendships',\n",
       " 'friggin',\n",
       " 'frodo',\n",
       " 'frog',\n",
       " 'from',\n",
       " 'fuck',\n",
       " 'fuckers',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'funner',\n",
       " 'funniest',\n",
       " 'funny',\n",
       " 'futile',\n",
       " 'future',\n",
       " 'fyi',\n",
       " 'gadgets',\n",
       " 'gaither',\n",
       " 'game',\n",
       " 'games',\n",
       " 'garrett',\n",
       " 'gary',\n",
       " 'gasp',\n",
       " 'gathered',\n",
       " 'gavin',\n",
       " 'gay',\n",
       " 'gayer',\n",
       " 'gayness',\n",
       " 'gays',\n",
       " 'geek',\n",
       " 'geisha',\n",
       " 'general',\n",
       " 'generalized',\n",
       " 'generally',\n",
       " 'generated',\n",
       " 'generation',\n",
       " 'genre',\n",
       " 'genres',\n",
       " 'georgia',\n",
       " 'german',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'gettting',\n",
       " 'giants',\n",
       " 'gift',\n",
       " 'gin',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'gl',\n",
       " 'glad',\n",
       " 'gladly',\n",
       " 'glitz',\n",
       " 'gn',\n",
       " 'go',\n",
       " 'goblet',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'google',\n",
       " 'gorgeous',\n",
       " 'gosh',\n",
       " 'gossip',\n",
       " 'got',\n",
       " 'goth',\n",
       " 'gotta',\n",
       " 'grabs',\n",
       " 'grand',\n",
       " 'great',\n",
       " 'grey',\n",
       " 'grips',\n",
       " 'groaning',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'grown',\n",
       " 'gun',\n",
       " 'guts',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gyllenhaal',\n",
       " 'gym',\n",
       " 'had',\n",
       " 'haha',\n",
       " 'hahaha',\n",
       " 'hahahaha',\n",
       " 'hahahahahaha',\n",
       " 'hahash',\n",
       " 'hair',\n",
       " 'haircut',\n",
       " 'hairy',\n",
       " 'half',\n",
       " 'halfway',\n",
       " 'hall',\n",
       " 'halle',\n",
       " 'halloween',\n",
       " 'halls',\n",
       " 'hammy',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'hanging',\n",
       " 'hank',\n",
       " 'hanks',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardcore',\n",
       " 'harder',\n",
       " 'harrison',\n",
       " 'harry',\n",
       " 'has',\n",
       " 'hat',\n",
       " 'hate',\n",
       " 'hated',\n",
       " 'hates',\n",
       " 'hating',\n",
       " 'haunt',\n",
       " 'haunted',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'headmistress',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heartbraking',\n",
       " 'heartbreaking',\n",
       " 'heath',\n",
       " 'heather',\n",
       " 'heavy',\n",
       " 'hedge',\n",
       " 'hell',\n",
       " 'hella',\n",
       " 'hello',\n",
       " 'helped',\n",
       " 'her',\n",
       " 'here',\n",
       " 'heresies',\n",
       " 'hermione',\n",
       " 'hero',\n",
       " 'het',\n",
       " 'heteronormativity',\n",
       " 'hey',\n",
       " 'hide',\n",
       " 'highly',\n",
       " 'hilarious',\n",
       " 'hill',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'hippie',\n",
       " 'hips',\n",
       " 'his',\n",
       " 'historical',\n",
       " 'hoffman',\n",
       " 'hogwarts',\n",
       " 'hogwash',\n",
       " 'hold',\n",
       " 'holding',\n",
       " 'hollywood',\n",
       " 'hollywoord',\n",
       " 'holy',\n",
       " 'home',\n",
       " 'homophobes',\n",
       " 'homophobic',\n",
       " 'homosexuality',\n",
       " 'honest',\n",
       " 'honestly',\n",
       " 'honor',\n",
       " 'hooked',\n",
       " 'hooker',\n",
       " 'hookup',\n",
       " 'hooray',\n",
       " 'hoot',\n",
       " 'hoover',\n",
       " 'hope',\n",
       " 'hopefully',\n",
       " 'horrible',\n",
       " 'horridly',\n",
       " 'horses',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'how',\n",
       " 'howard',\n",
       " 'however',\n",
       " 'hp',\n",
       " 'huge',\n",
       " 'hugged',\n",
       " 'hugh',\n",
       " 'huh',\n",
       " 'humor',\n",
       " 'hung',\n",
       " 'husband',\n",
       " 'hype',\n",
       " 'hyped',\n",
       " 'ian',\n",
       " 'icons',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'idiot',\n",
       " 'idiots',\n",
       " 'idk',\n",
       " 'if',\n",
       " 'ignorant',\n",
       " 'ignore',\n",
       " 'iii',\n",
       " 'ike',\n",
       " 'illegally',\n",
       " 'illustrated',\n",
       " 'im',\n",
       " 'image',\n",
       " 'images',\n",
       " 'imagine',\n",
       " 'immediately',\n",
       " 'immensely',\n",
       " 'immortal',\n",
       " 'imo',\n",
       " 'important',\n",
       " 'impossible',\n",
       " 'impressive',\n",
       " 'in',\n",
       " 'inaccurate',\n",
       " 'inappropriate',\n",
       " 'included',\n",
       " 'including',\n",
       " 'increasing',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'independant',\n",
       " 'independent',\n",
       " 'indian',\n",
       " 'indicative',\n",
       " 'indoctrinate',\n",
       " 'industry',\n",
       " 'infiltrate',\n",
       " 'infuser',\n",
       " 'inherently',\n",
       " 'insane',\n",
       " 'insanely',\n",
       " 'inside',\n",
       " 'inspired',\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gays',\n",
       " 'along',\n",
       " 'giving',\n",
       " 'academy',\n",
       " 'van',\n",
       " 'song',\n",
       " 'nc',\n",
       " 'wanted',\n",
       " 'cake',\n",
       " 'schools']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_features = count_vectorizer.transform( train_ds.text )\n",
    "type(train_ds_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6918, 2132)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the matrix to a dataframe\n",
    "train_ds_df = pd.DataFrame(train_ds_features.todense())\n",
    "# Setting the column names to the features i.e. words\n",
    "train_ds_df.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>007</th>\n",
       "      <th>10</th>\n",
       "      <th>10pm</th>\n",
       "      <th>12</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>1984</th>\n",
       "      <th>1st</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>yip</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>your</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yuh</th>\n",
       "      <th>zach</th>\n",
       "      <th>zen</th>\n",
       "      <th>µª</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 2132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  007  10  10pm  12  16  17  1984  1st  200  ...  yip  you  young  \\\n",
       "0      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "1      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "2      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "3      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "4      0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "...   ..  ...  ..   ...  ..  ..  ..   ...  ...  ...  ...  ...  ...    ...   \n",
       "6913   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "6914   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "6915   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "6916   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "6917   0    0   0     0   0   0   0     0    0    0  ...    0    0      0   \n",
       "\n",
       "      younger  your  yuck  yuh  zach  zen  µª  \n",
       "0           0     0     0    0     0    0   0  \n",
       "1           0     0     0    0     0    0   0  \n",
       "2           0     0     0    0     0    0   0  \n",
       "3           0     0     0    0     0    0   0  \n",
       "4           0     0     0    0     0    0   0  \n",
       "...       ...   ...   ...  ...   ...  ...  ..  \n",
       "6913        0     0     0    0     0    0   0  \n",
       "6914        0     0     0    0     0    0   0  \n",
       "6915        0     0     0    0     0    0   0  \n",
       "6916        0     0     0    0     0    0   0  \n",
       "6917        0     0     0    0     0    0   0  \n",
       "\n",
       "[6918 rows x 2132 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                     text\n",
       "0          1  The Da Vinci Code book is just awesome."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>away</th>\n",
       "      <th>awesome</th>\n",
       "      <th>awesomely</th>\n",
       "      <th>awesomeness</th>\n",
       "      <th>awesomest</th>\n",
       "      <th>awful</th>\n",
       "      <th>awkward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   away  awesome  awesomely  awesomeness  awesomest  awful  awkward\n",
       "0     0        1          0            0          0      0        0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_df.iloc[0:1, 150:157]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>da</th>\n",
       "      <th>vinci</th>\n",
       "      <th>code</th>\n",
       "      <th>book</th>\n",
       "      <th>is</th>\n",
       "      <th>just</th>\n",
       "      <th>awesome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the  da  vinci  code  book  is  just  awesome\n",
       "0    1   1      1     1     1   1     1        1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_df[['the', 'da', \"vinci\", \"code\", \"book\", 'is', 'just', 'awesome']][0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extarct relivant words which are having atleast good amount of frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summing up the occurances of features column wise\n",
    "features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n",
    "feature_counts_df = pd.DataFrame( dict( features = features,counts = features_counts ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure( figsize=(12,5))\n",
    "plt.hist(feature_counts_df.counts, bins=50, range = (0, 2000));\n",
    "plt.xlabel( 'Frequency of words' )\n",
    "plt.ylabel( 'Density' );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_counts_df[feature_counts_df.counts == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "# Create the dictionary from the corpus\n",
    "feature_vector = count_vectorizer.fit( train_ds.text )\n",
    "# Get the feature names\n",
    "features = feature_vector.get_feature_names()\n",
    "# Transform the document into vectors\n",
    "train_ds_features = count_vectorizer.transform( train_ds.text )\n",
    "# Count the frequency of the features\n",
    "features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n",
    "feature_counts = pd.DataFrame( dict( features = features,counts = features_counts ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>the</td>\n",
       "      <td>3306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>and</td>\n",
       "      <td>2154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>harry</td>\n",
       "      <td>2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>potter</td>\n",
       "      <td>2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>code</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>vinci</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>da</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>mountain</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>brokeback</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>love</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>is</td>\n",
       "      <td>1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>was</td>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>mission</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>impossible</td>\n",
       "      <td>1093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       features  counts\n",
       "866         the    3306\n",
       "37          and    2154\n",
       "358       harry    2093\n",
       "675      potter    2093\n",
       "138        code    2002\n",
       "934       vinci    2001\n",
       "178          da    2001\n",
       "528    mountain    2000\n",
       "104   brokeback    2000\n",
       "488        love    1624\n",
       "423          is    1520\n",
       "941         was    1176\n",
       "60      awesome    1127\n",
       "521     mission    1094\n",
       "413  impossible    1093"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counts.sort_values('counts',ascending = False)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove STOPWORD like is, am are , the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few stop words:  ['ie', 'ten', 'he', 'were', 'hereby', 'nor', 'was', 'find', 'amount', 'whereupon']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "#Printing first few stop words\n",
    "print(\"Few stop words: \", list(my_stop_words)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding custom words to the list of stop words\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union( ['harry', 'potter', 'code', 'vinci', 'da',\n",
    "'harri', 'mountain', 'movie', 'movies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting stop words list\n",
    "count_vectorizer = CountVectorizer( stop_words = my_stop_words, max_features = 1000 )\n",
    "feature_vector = count_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = count_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()\n",
    "features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n",
    "feature_counts = pd.DataFrame( dict( features = features,\n",
    "counts = features_counts ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>brokeback</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>love</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>mission</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>impossible</td>\n",
       "      <td>1093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>like</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>sucks</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>sucked</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>hate</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>really</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>stupid</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>just</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>know</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>suck</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>loved</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       features  counts\n",
       "73    brokeback    2000\n",
       "408        love    1624\n",
       "39      awesome    1127\n",
       "436     mission    1094\n",
       "341  impossible    1093\n",
       "390        like     974\n",
       "745       sucks     602\n",
       "743      sucked     600\n",
       "297        hate     578\n",
       "652      really     374\n",
       "741      stupid     365\n",
       "362        just     287\n",
       "374        know     276\n",
       "742        suck     276\n",
       "409       loved     256"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counts.sort_values( \"counts\", ascending = False )[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning - Stemming or Lemmatization\n",
    "\n",
    "### To get words into root form and hence in a motivation of decreasing few more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. PorterStemmer\n",
    "#2. LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "\n",
    "#Custom function for stemming and stop word removal\n",
    "def stemmed_words(doc):\n",
    "    ### Stemming of words\n",
    "    stemmed_words = (stemmer.stem(w) for w in analyzer(doc))\n",
    "    ### Remove the words in stop words list\n",
    "    non_stop_words = [ word for word in list(set(stemmed_words) - set(my_stop_words)) ]\n",
    "    return non_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>brokeback</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>love</td>\n",
       "      <td>1837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>suck</td>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>wa</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>awesom</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>mission</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>imposs</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>movi</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>like</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>hate</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>becaus</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>realli</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>stupid</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>know</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>read</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      features  counts\n",
       "80   brokeback    1930\n",
       "406       love    1837\n",
       "801       suck    1378\n",
       "922         wa    1142\n",
       "43      awesom    1116\n",
       "432    mission    1090\n",
       "344     imposs    1090\n",
       "438       movi    1052\n",
       "392       like     823\n",
       "298       hate     636\n",
       "54      becaus     524\n",
       "602     realli     370\n",
       "794     stupid     364\n",
       "378       know     354\n",
       "597       read     284"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer( analyzer=stemmed_words, max_features = 1000)\n",
    "feature_vector = count_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = count_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()\n",
    "features_counts = np.sum( train_ds_features.toarray(), axis = 0 )\n",
    "feature_counts = pd.DataFrame( dict( features = features,\n",
    "counts = features_counts ) )\n",
    "feature_counts.sort_values( \"counts\", ascending = False )[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the document vector matrix into dataframe\n",
    "train_ds_df = pd.DataFrame(train_ds_features.todense())\n",
    "# Assign the features names to the column\n",
    "train_ds_df.columns = features\n",
    "# Assign the sentiment labels to the train_ds\n",
    "train_ds_df['sentiment'] = train_ds.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>17</th>\n",
       "      <th>33</th>\n",
       "      <th>6th</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>absurd</th>\n",
       "      <th>academi</th>\n",
       "      <th>accept</th>\n",
       "      <th>accompani</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yip</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yuh</th>\n",
       "      <th>zach</th>\n",
       "      <th>zen</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      10  17  33  6th  abl  absolut  absurd  academi  accept  accompani  ...  \\\n",
       "0      0   0   0    0    0        0       0        0       0          0  ...   \n",
       "1      0   0   0    0    0        0       0        0       0          0  ...   \n",
       "2      0   0   0    0    0        0       0        0       0          0  ...   \n",
       "3      0   0   0    0    0        0       0        0       0          0  ...   \n",
       "4      0   0   0    0    0        0       0        0       0          0  ...   \n",
       "...   ..  ..  ..  ...  ...      ...     ...      ...     ...        ...  ...   \n",
       "6913   0   0   0    0    0        0       0        0       0          0  ...   \n",
       "6914   0   0   0    0    0        0       0        0       0          0  ...   \n",
       "6915   0   0   0    0    0        0       0        0       0          0  ...   \n",
       "6916   0   0   0    0    0        0       0        0       0          0  ...   \n",
       "6917   0   0   0    0    0        0       0        0       0          0  ...   \n",
       "\n",
       "      year  yesterday  yip  young  younger  yuck  yuh  zach  zen  sentiment  \n",
       "0        0          0    0      0        0     0    0     0    0          1  \n",
       "1        0          0    0      0        0     0    0     0    0          1  \n",
       "2        0          0    0      0        0     0    0     0    0          1  \n",
       "3        0          0    0      0        0     0    0     0    0          1  \n",
       "4        0          0    0      0        0     0    0     0    0          1  \n",
       "...    ...        ...  ...    ...      ...   ...  ...   ...  ...        ...  \n",
       "6913     0          0    0      0        0     0    0     0    0          0  \n",
       "6914     0          0    0      0        0     0    0     0    0          0  \n",
       "6915     0          0    0      0        0     0    0     0    0          0  \n",
       "6916     0          0    0      0        0     0    0     0    0          0  \n",
       "6917     0          0    0      0        0     0    0     0    0          0  \n",
       "\n",
       "[6918 rows x 1001 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split( train_ds_features,train_ds.sentiment,test_size = 0.3,random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit( train_X.toarray(), train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_predicted = nb_clf.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       873\n",
      "           1       0.98      0.99      0.98      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.98      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = nb_clf\n",
    "model_name = \"Binomial Naive Bayes Classifier\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = logreg.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      1.00      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = logreg\n",
    "model_name = \"Logistic Regression\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "decision_tree.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = decision_tree.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "0  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  \n",
       "0                    0.031305  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = decision_tree\n",
    "model_name = \"Decission Tree\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = random_forest.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "0  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "0  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  \n",
       "0                    0.031305  \n",
       "0                    0.025100  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = random_forest\n",
    "model_name = \"Random Forest\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = xgboost.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "0  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "0  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "0  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  \n",
       "0                    0.031305  \n",
       "0                    0.025100  \n",
       "0                    0.019235  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = xgboost\n",
    "model_name = \"XG Boost\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "sgd = OneVsRestClassifier(SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = sgd.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       873\n",
      "           1       0.99      1.00      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.015166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "0  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "0  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "0  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "0  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "0  [0.96, 0.98, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  \n",
       "0                    0.031305  \n",
       "0                    0.025100  \n",
       "0                    0.019235  \n",
       "0                    0.015166  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = sgd\n",
    "model_name = \"Stochastic Gradient Descent\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "gausian_process = GaussianProcessClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "gausian_process.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = gausian_process.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       873\n",
      "           1       0.98      1.00      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.98      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.015166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "0  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "0  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "0  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "0  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "0  [0.96, 0.98, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  \n",
       "0                    0.031305  \n",
       "0                    0.025100  \n",
       "0                    0.019235  \n",
       "0                    0.015166  \n",
       "0                    0.025100  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = gausian_process\n",
    "model_name = \"Gausian Process\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = knn.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       873\n",
      "           1       0.97      1.00      0.99      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.99      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.015166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "0  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "0  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "0  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "0  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "0  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "0  [0.96, 0.98, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  \n",
       "0                    0.031305  \n",
       "0                    0.025100  \n",
       "0                    0.019235  \n",
       "0                    0.015166  \n",
       "0                    0.025100  \n",
       "0                    0.032094  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = knn\n",
    "model_name = \"K Nearst Neighbour\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = lda.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       873\n",
      "           1       0.99      0.99      0.99      1203\n",
      "\n",
      "    accuracy                           0.99      2076\n",
      "   macro avg       0.99      0.99      0.99      2076\n",
      "weighted avg       0.99      0.99      0.99      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.015166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "0  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "0  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "0  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "0  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "0  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "0  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "0  [0.96, 0.98, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  \n",
       "0                    0.031305  \n",
       "0                    0.025100  \n",
       "0                    0.019235  \n",
       "0                    0.015166  \n",
       "0                    0.025100  \n",
       "0                    0.032094  \n",
       "0                    0.007071  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = lda\n",
    "model_name = \"Linear Discriminant Analysis\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(train_X.toarray(), train_y)\n",
    "test_ds_predicted = svm.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       873\n",
      "           1       0.97      0.99      0.98      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.98      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Binomial Naive Bayes Classifier</td>\n",
       "      <td>[0.92, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.95, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Decission Tree</td>\n",
       "      <td>[0.92, 0.98, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.031305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>XG Boost</td>\n",
       "      <td>[0.95, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>[0.96, 0.98, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.96-1.0</td>\n",
       "      <td>0.015166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Gausian Process</td>\n",
       "      <td>[0.94, 0.99, 0.99, 1.0, 1.0]</td>\n",
       "      <td>0.94-1.0</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>K Nearst Neighbour</td>\n",
       "      <td>[0.92, 0.97, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.92-1.0</td>\n",
       "      <td>0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Linear Discriminant Analysis</td>\n",
       "      <td>[0.98, 0.99, 0.99, 0.99, 1.0]</td>\n",
       "      <td>0.98-1.0</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag Of Words with NLTK Stemming</td>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>[0.95, 0.97, 0.98, 0.99, 1.0]</td>\n",
       "      <td>0.95-1.0</td>\n",
       "      <td>0.019235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Process                       Model Name  \\\n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming  Binomial Naive Bayes Classifier   \n",
       "0  Bag Of Words with NLTK Stemming              Logistic Regression   \n",
       "0  Bag Of Words with NLTK Stemming                   Decission Tree   \n",
       "0  Bag Of Words with NLTK Stemming                    Random Forest   \n",
       "0  Bag Of Words with NLTK Stemming                         XG Boost   \n",
       "0  Bag Of Words with NLTK Stemming      Stochastic Gradient Descent   \n",
       "0  Bag Of Words with NLTK Stemming                  Gausian Process   \n",
       "0  Bag Of Words with NLTK Stemming               K Nearst Neighbour   \n",
       "0  Bag Of Words with NLTK Stemming     Linear Discriminant Analysis   \n",
       "0  Bag Of Words with NLTK Stemming           Support Vector Machine   \n",
       "\n",
       "                       F1 Scores Range of F1 Scores  \\\n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.92, 0.98, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.95, 0.99, 0.99, 1.0, 1.0]           0.95-1.0   \n",
       "0  [0.92, 0.98, 0.98, 0.99, 1.0]           0.92-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.95, 0.98, 0.99, 0.99, 1.0]           0.95-1.0   \n",
       "0  [0.96, 0.98, 0.99, 0.99, 1.0]           0.96-1.0   \n",
       "0   [0.94, 0.99, 0.99, 1.0, 1.0]           0.94-1.0   \n",
       "0  [0.92, 0.97, 0.99, 0.99, 1.0]           0.92-1.0   \n",
       "0  [0.98, 0.99, 0.99, 0.99, 1.0]           0.98-1.0   \n",
       "0  [0.95, 0.97, 0.98, 0.99, 1.0]           0.95-1.0   \n",
       "\n",
       "   Std Deviation of F1 Scores  \n",
       "0                    0.032094  \n",
       "0                    0.032094  \n",
       "0                    0.020736  \n",
       "0                    0.031305  \n",
       "0                    0.025100  \n",
       "0                    0.019235  \n",
       "0                    0.015166  \n",
       "0                    0.025100  \n",
       "0                    0.032094  \n",
       "0                    0.007071  \n",
       "0                    0.019235  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = svm\n",
    "model_name = \"Support Vector Machine\"\n",
    "process = \"Bag Of Words with NLTK Stemming\"\n",
    "n_splits = 5\n",
    "X = train_ds_features.toarray()\n",
    "y = train_ds.sentiment\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_selection.to_csv(\"Model_statistics.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer( analyzer=stemmed_words,max_features = 1000)\n",
    "\n",
    "\n",
    "feature_vector = tfidf_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = tfidf_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '17',\n",
       " '33',\n",
       " '6th',\n",
       " 'abl',\n",
       " 'absolut',\n",
       " 'absurd',\n",
       " 'academi',\n",
       " 'accept',\n",
       " 'accompani',\n",
       " 'ach',\n",
       " 'acn',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'admir',\n",
       " 'ador',\n",
       " 'adult',\n",
       " 'ago',\n",
       " 'agre',\n",
       " 'alreadi',\n",
       " 'alway',\n",
       " 'amaz',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'ani',\n",
       " 'anim',\n",
       " 'anyon',\n",
       " 'anyth',\n",
       " 'appar',\n",
       " 'appeal',\n",
       " 'articl',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'attempt',\n",
       " 'attract',\n",
       " 'audrey',\n",
       " 'author',\n",
       " 'aw',\n",
       " 'award',\n",
       " 'awesom',\n",
       " 'awesomest',\n",
       " 'azkaban',\n",
       " 'bad',\n",
       " 'ball',\n",
       " 'ban',\n",
       " 'bang',\n",
       " 'basic',\n",
       " 'bean',\n",
       " 'beat',\n",
       " 'beauti',\n",
       " 'becaus',\n",
       " 'becom',\n",
       " 'befor',\n",
       " 'begin',\n",
       " 'believ',\n",
       " 'besid',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bias',\n",
       " 'big',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'blond',\n",
       " 'blood',\n",
       " 'board',\n",
       " 'bobbypin',\n",
       " 'bodi',\n",
       " 'bogu',\n",
       " 'bonker',\n",
       " 'book',\n",
       " 'bore',\n",
       " 'bought',\n",
       " 'boycot',\n",
       " 'brilliant',\n",
       " 'brokeback',\n",
       " 'brown',\n",
       " 'btw',\n",
       " 'bullshit',\n",
       " 'butt',\n",
       " 'buy',\n",
       " 'bye',\n",
       " 'came',\n",
       " 'capot',\n",
       " 'car',\n",
       " 'care',\n",
       " 'case',\n",
       " 'catch',\n",
       " 'catcher',\n",
       " 'challeng',\n",
       " 'chang',\n",
       " 'charact',\n",
       " 'children',\n",
       " 'chines',\n",
       " 'choic',\n",
       " 'chri',\n",
       " 'christian',\n",
       " 'christma',\n",
       " 'class',\n",
       " 'clean',\n",
       " 'club',\n",
       " 'cock',\n",
       " 'cocktail',\n",
       " 'cold',\n",
       " 'colleg',\n",
       " 'colour',\n",
       " 'combin',\n",
       " 'come',\n",
       " 'comment',\n",
       " 'commun',\n",
       " 'compar',\n",
       " 'conclus',\n",
       " 'conquer',\n",
       " 'consid',\n",
       " 'controversi',\n",
       " 'convers',\n",
       " 'cool',\n",
       " 'copi',\n",
       " 'costum',\n",
       " 'count',\n",
       " 'coupl',\n",
       " 'cours',\n",
       " 'cowboy',\n",
       " 'coz',\n",
       " 'crap',\n",
       " 'crappi',\n",
       " 'crash',\n",
       " 'craze',\n",
       " 'crazi',\n",
       " 'creatur',\n",
       " 'cri',\n",
       " 'cring',\n",
       " 'critic',\n",
       " 'cruis',\n",
       " 'cultur',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'dad',\n",
       " 'damn',\n",
       " 'dan',\n",
       " 'danc',\n",
       " 'daniel',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'day',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'decent',\n",
       " 'decid',\n",
       " 'deep',\n",
       " 'defin',\n",
       " 'definit',\n",
       " 'dementor',\n",
       " 'demon',\n",
       " 'depress',\n",
       " 'deserv',\n",
       " 'desper',\n",
       " 'despis',\n",
       " 'dick',\n",
       " 'dictat',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'differ',\n",
       " 'disappoint',\n",
       " 'discuss',\n",
       " 'dislik',\n",
       " 'disney',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'don',\n",
       " 'donkey',\n",
       " 'dont',\n",
       " 'dork',\n",
       " 'draco',\n",
       " 'drag',\n",
       " 'dragon',\n",
       " 'draw',\n",
       " 'dream',\n",
       " 'dress',\n",
       " 'drive',\n",
       " 'dudee',\n",
       " 'dumb',\n",
       " 'dure',\n",
       " 'dvd',\n",
       " 'eat',\n",
       " 'educ',\n",
       " 'egg',\n",
       " 'els',\n",
       " 'emma',\n",
       " 'empti',\n",
       " 'end',\n",
       " 'enjoy',\n",
       " 'equal',\n",
       " 'eragon',\n",
       " 'erm',\n",
       " 'escapad',\n",
       " 'especi',\n",
       " 'event',\n",
       " 'everi',\n",
       " 'everybodi',\n",
       " 'everyon',\n",
       " 'everyth',\n",
       " 'evil',\n",
       " 'exampl',\n",
       " 'excel',\n",
       " 'excit',\n",
       " 'expect',\n",
       " 'explain',\n",
       " 'exquisit',\n",
       " 'extrem',\n",
       " 'eye',\n",
       " 'eyr',\n",
       " 'fabul',\n",
       " 'fact',\n",
       " 'fall',\n",
       " 'fan',\n",
       " 'fandom',\n",
       " 'fanfic',\n",
       " 'fanfict',\n",
       " 'fantasi',\n",
       " 'far',\n",
       " 'fat',\n",
       " 'fault',\n",
       " 'favor',\n",
       " 'favorit',\n",
       " 'favourit',\n",
       " 'feel',\n",
       " 'felicia',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'fic',\n",
       " 'figur',\n",
       " 'film',\n",
       " 'final',\n",
       " 'finish',\n",
       " 'firework',\n",
       " 'fit',\n",
       " 'flick',\n",
       " 'food',\n",
       " 'forgotten',\n",
       " 'form',\n",
       " 'franchis',\n",
       " 'freak',\n",
       " 'freakin',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friendship',\n",
       " 'fuck',\n",
       " 'fun',\n",
       " 'funni',\n",
       " 'funniest',\n",
       " 'futur',\n",
       " 'gaither',\n",
       " 'game',\n",
       " 'gari',\n",
       " 'gay',\n",
       " 'geek',\n",
       " 'gener',\n",
       " 'genr',\n",
       " 'georgia',\n",
       " 'gift',\n",
       " 'gin',\n",
       " 'girl',\n",
       " 'given',\n",
       " 'glad',\n",
       " 'goblet',\n",
       " 'god',\n",
       " 'goin',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'gorgeou',\n",
       " 'got',\n",
       " 'goth',\n",
       " 'gotta',\n",
       " 'grab',\n",
       " 'great',\n",
       " 'groan',\n",
       " 'gun',\n",
       " 'guy',\n",
       " 'ha',\n",
       " 'half',\n",
       " 'hall',\n",
       " 'halloween',\n",
       " 'hand',\n",
       " 'hank',\n",
       " 'happen',\n",
       " 'happi',\n",
       " 'hard',\n",
       " 'hardcor',\n",
       " 'hat',\n",
       " 'hate',\n",
       " 'haunt',\n",
       " 'haven',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heath',\n",
       " 'hedg',\n",
       " 'hell',\n",
       " 'hella',\n",
       " 'help',\n",
       " 'hero',\n",
       " 'het',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'hide',\n",
       " 'hill',\n",
       " 'hip',\n",
       " 'hoffman',\n",
       " 'hogwart',\n",
       " 'hold',\n",
       " 'hollywood',\n",
       " 'home',\n",
       " 'homophob',\n",
       " 'homosexu',\n",
       " 'honor',\n",
       " 'hoot',\n",
       " 'hoover',\n",
       " 'hope',\n",
       " 'horribl',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hous',\n",
       " 'hp',\n",
       " 'hung',\n",
       " 'hype',\n",
       " 'icon',\n",
       " 'idea',\n",
       " 'idiot',\n",
       " 'idk',\n",
       " 'ignor',\n",
       " 'iii',\n",
       " 'im',\n",
       " 'imag',\n",
       " 'immedi',\n",
       " 'imposs',\n",
       " 'inaccur',\n",
       " 'includ',\n",
       " 'incred',\n",
       " 'independ',\n",
       " 'industri',\n",
       " 'insan',\n",
       " 'instead',\n",
       " 'invis',\n",
       " 'involv',\n",
       " 'isn',\n",
       " 'issu',\n",
       " 'ive',\n",
       " 'jack',\n",
       " 'jake',\n",
       " 'jane',\n",
       " 'jesu',\n",
       " 'job',\n",
       " 'john',\n",
       " 'johnni',\n",
       " 'join',\n",
       " 'joke',\n",
       " 'just',\n",
       " 'kate',\n",
       " 'kelsi',\n",
       " 'key',\n",
       " 'kick',\n",
       " 'kid',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kirsten',\n",
       " 'kiss',\n",
       " 'knew',\n",
       " 'knight',\n",
       " 'know',\n",
       " 'la',\n",
       " 'lah',\n",
       " 'laid',\n",
       " 'lame',\n",
       " 'laugh',\n",
       " 'leah',\n",
       " 'lee',\n",
       " 'left',\n",
       " 'let',\n",
       " 'level',\n",
       " 'librari',\n",
       " 'lie',\n",
       " 'life',\n",
       " 'like',\n",
       " 'listen',\n",
       " 'lit',\n",
       " 'littl',\n",
       " 'live',\n",
       " 'll',\n",
       " 'loath',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lord',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lousi',\n",
       " 'love',\n",
       " 'lubb',\n",
       " 'luck',\n",
       " 'luv',\n",
       " 'lynn',\n",
       " 'magic',\n",
       " 'main',\n",
       " 'mainstream',\n",
       " 'major',\n",
       " 'majorli',\n",
       " 'make',\n",
       " 'malfoy',\n",
       " 'man',\n",
       " 'mani',\n",
       " 'marcia',\n",
       " 'materi',\n",
       " 'matter',\n",
       " 'mayb',\n",
       " 'mean',\n",
       " 'meet',\n",
       " 'men',\n",
       " 'mention',\n",
       " 'mi3',\n",
       " 'minut',\n",
       " 'mirror',\n",
       " 'miss',\n",
       " 'mission',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'month',\n",
       " 'mother',\n",
       " 'mouth',\n",
       " 'movi',\n",
       " 'mr',\n",
       " 'mtv',\n",
       " 'murderbal',\n",
       " 'music',\n",
       " 'narnia',\n",
       " 'nc',\n",
       " 'nearli',\n",
       " 'need',\n",
       " 'nerd',\n",
       " 'new',\n",
       " 'news',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nois',\n",
       " 'normal',\n",
       " 'noth',\n",
       " 'novel',\n",
       " 'offens',\n",
       " 'offici',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'omg',\n",
       " 'onc',\n",
       " 'oni',\n",
       " 'onli',\n",
       " 'onlin',\n",
       " 'ootp',\n",
       " 'open',\n",
       " 'opinion',\n",
       " 'optimu',\n",
       " 'oreo',\n",
       " 'orig',\n",
       " 'oscar',\n",
       " 'ossana',\n",
       " 'otp',\n",
       " 'otter',\n",
       " 'outnumb',\n",
       " 'outshin',\n",
       " 'outsid',\n",
       " 'outta',\n",
       " 'overal',\n",
       " 'overcom',\n",
       " 'overexager',\n",
       " 'overlook',\n",
       " 'oversimplifi',\n",
       " 'overslept',\n",
       " 'pack',\n",
       " 'packag',\n",
       " 'page',\n",
       " 'pale',\n",
       " 'pant',\n",
       " 'panti',\n",
       " 'paper',\n",
       " 'parent',\n",
       " 'park',\n",
       " 'parti',\n",
       " 'partyin',\n",
       " 'passion',\n",
       " 'past',\n",
       " 'patirot',\n",
       " 'paul',\n",
       " 'pc',\n",
       " 'pegg',\n",
       " 'peopl',\n",
       " 'perfect',\n",
       " 'perform',\n",
       " 'perhap',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personali',\n",
       " 'phase',\n",
       " 'phenomenon',\n",
       " 'phillip',\n",
       " 'philosoph',\n",
       " 'phoenix',\n",
       " 'phone',\n",
       " 'photographi',\n",
       " 'phrase',\n",
       " 'pic',\n",
       " 'picard',\n",
       " 'picki',\n",
       " 'picnic',\n",
       " 'pictur',\n",
       " 'picturesqu',\n",
       " 'piec',\n",
       " 'pink',\n",
       " 'pirat',\n",
       " 'place',\n",
       " 'plain',\n",
       " 'plan',\n",
       " 'plastic',\n",
       " 'plausibl',\n",
       " 'play',\n",
       " 'pleas',\n",
       " 'plot',\n",
       " 'plu',\n",
       " 'pocket',\n",
       " 'poem',\n",
       " 'point',\n",
       " 'polic',\n",
       " 'polit',\n",
       " 'poorli',\n",
       " 'pop',\n",
       " 'popular',\n",
       " 'portug',\n",
       " 'portugues',\n",
       " 'poseidon',\n",
       " 'posit',\n",
       " 'possibl',\n",
       " 'possum',\n",
       " 'post',\n",
       " 'postpon',\n",
       " 'potterhol',\n",
       " 'power',\n",
       " 'preciou',\n",
       " 'predict',\n",
       " 'present',\n",
       " 'press',\n",
       " 'pretend',\n",
       " 'pretti',\n",
       " 'preview',\n",
       " 'primari',\n",
       " 'prime',\n",
       " 'princ',\n",
       " 'prison',\n",
       " 'prize',\n",
       " 'probabl',\n",
       " 'problem',\n",
       " 'professor',\n",
       " 'profound',\n",
       " 'project',\n",
       " 'protest',\n",
       " 'proud',\n",
       " 'ps',\n",
       " 'psycholog',\n",
       " 'public',\n",
       " 'publicli',\n",
       " 'pud',\n",
       " 'pull',\n",
       " 'pup',\n",
       " 'purchas',\n",
       " 'quaintli',\n",
       " 'queen',\n",
       " 'queer',\n",
       " 'question',\n",
       " 'quick',\n",
       " 'quip',\n",
       " 'quirki',\n",
       " 'quit',\n",
       " 'quiz',\n",
       " 'quizz',\n",
       " 'racism',\n",
       " 'ran',\n",
       " 'rant',\n",
       " 'react',\n",
       " 'reaction',\n",
       " 'read',\n",
       " 'reader',\n",
       " 'real',\n",
       " 'realiti',\n",
       " 'realiz',\n",
       " 'realli',\n",
       " 'reason',\n",
       " 'receiv',\n",
       " 'recent',\n",
       " 'record',\n",
       " 'refer',\n",
       " 'refus',\n",
       " 'regardless',\n",
       " 'rehears',\n",
       " 'relat',\n",
       " 'relax',\n",
       " 'releas',\n",
       " 'relic',\n",
       " 'religi',\n",
       " 'religion',\n",
       " 'remind',\n",
       " 'remix',\n",
       " 'rent',\n",
       " 'reopen',\n",
       " 'repli',\n",
       " 'request',\n",
       " 'respect',\n",
       " 'rest',\n",
       " 'retart',\n",
       " 'review',\n",
       " 'ride',\n",
       " 'right',\n",
       " 'ring',\n",
       " 'rob',\n",
       " 'rock',\n",
       " 'ron',\n",
       " 'row',\n",
       " 'rowl',\n",
       " 'royal',\n",
       " 'rp',\n",
       " 'ruin',\n",
       " 'rule',\n",
       " 'run',\n",
       " 'runaway',\n",
       " 'runner',\n",
       " 'russotti',\n",
       " 'rv',\n",
       " 'sad',\n",
       " 'said',\n",
       " 'sake',\n",
       " 'sale',\n",
       " 'sam',\n",
       " 'sarcast',\n",
       " 'saturday',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'sawyer',\n",
       " 'say',\n",
       " 'scar',\n",
       " 'scare',\n",
       " 'scarf',\n",
       " 'scenario',\n",
       " 'scene',\n",
       " 'sceneri',\n",
       " 'scent',\n",
       " 'school',\n",
       " 'scientolog',\n",
       " 'scientologist',\n",
       " 'scifi',\n",
       " 'score',\n",
       " 'screen',\n",
       " 'screenplay',\n",
       " 'screw',\n",
       " 'sean',\n",
       " 'search',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'section',\n",
       " 'seek',\n",
       " 'seen',\n",
       " 'selfish',\n",
       " 'semest',\n",
       " 'sens',\n",
       " 'sent',\n",
       " 'sentri',\n",
       " 'sequel',\n",
       " 'seri',\n",
       " 'seriou',\n",
       " 'set',\n",
       " 'settin',\n",
       " 'sexi',\n",
       " 'sexual',\n",
       " 'seymor',\n",
       " 'seymour',\n",
       " 'shade',\n",
       " 'shadeslay',\n",
       " 'shame',\n",
       " 'shape',\n",
       " 'share',\n",
       " 'shatter',\n",
       " 'shell',\n",
       " 'shield',\n",
       " 'ship',\n",
       " 'shipmat',\n",
       " 'shit',\n",
       " 'shitti',\n",
       " 'shittiest',\n",
       " 'shoe',\n",
       " 'shop',\n",
       " 'short',\n",
       " 'shout',\n",
       " 'showcas',\n",
       " 'shraddha',\n",
       " 'shut',\n",
       " 'si',\n",
       " 'sick',\n",
       " 'sign',\n",
       " 'silent',\n",
       " 'silli',\n",
       " 'silver',\n",
       " 'simmon',\n",
       " 'simon',\n",
       " 'simpl',\n",
       " 'simpli',\n",
       " 'sinc',\n",
       " 'sing',\n",
       " 'sister',\n",
       " 'sit',\n",
       " 'sivullinen',\n",
       " 'sixth',\n",
       " 'skin',\n",
       " 'sky',\n",
       " 'slap',\n",
       " 'slash',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smoke',\n",
       " 'snow',\n",
       " 'snuck',\n",
       " 'sold',\n",
       " 'someon',\n",
       " 'someth',\n",
       " 'sometim',\n",
       " 'song',\n",
       " 'soo',\n",
       " 'soon',\n",
       " 'sooo',\n",
       " 'soooo',\n",
       " 'soooooo',\n",
       " 'soooooooo',\n",
       " 'sorcer',\n",
       " 'sorri',\n",
       " 'sort',\n",
       " 'soul',\n",
       " 'sound',\n",
       " 'soundtrack',\n",
       " 'south',\n",
       " 'spanish',\n",
       " 'speak',\n",
       " 'speaker',\n",
       " 'spec',\n",
       " 'special',\n",
       " 'specif',\n",
       " 'spectacularli',\n",
       " 'spell',\n",
       " 'spend',\n",
       " 'spi',\n",
       " 'spin',\n",
       " 'spine',\n",
       " 'spite',\n",
       " 'spoke',\n",
       " 'spontan',\n",
       " 'sport',\n",
       " 'springer',\n",
       " 'stand',\n",
       " 'standpoint',\n",
       " 'star',\n",
       " 'start',\n",
       " 'state',\n",
       " 'station',\n",
       " 'stay',\n",
       " 'stink',\n",
       " 'stinkin',\n",
       " 'stitch',\n",
       " 'stite',\n",
       " 'stone',\n",
       " 'stop',\n",
       " 'stori',\n",
       " 'storytim',\n",
       " 'straight',\n",
       " 'strang',\n",
       " 'street',\n",
       " 'strip',\n",
       " 'struggl',\n",
       " 'student',\n",
       " 'studi',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'stupidest',\n",
       " 'style',\n",
       " 'su',\n",
       " 'subject',\n",
       " 'subtitl',\n",
       " 'success',\n",
       " 'suck',\n",
       " 'sucki',\n",
       " 'sue',\n",
       " 'suicid',\n",
       " 'sum',\n",
       " 'summer',\n",
       " 'super',\n",
       " 'support',\n",
       " 'suppos',\n",
       " 'sure',\n",
       " 'tabl',\n",
       " 'tale',\n",
       " 'talk',\n",
       " 'tautou',\n",
       " 'tc',\n",
       " 'teach',\n",
       " 'tell',\n",
       " 'terribl',\n",
       " 'thank',\n",
       " 'theater',\n",
       " 'theme',\n",
       " 'thesi',\n",
       " 'thi',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thirdli',\n",
       " 'tho',\n",
       " 'thought',\n",
       " 'thousand',\n",
       " 'threw',\n",
       " 'thriller',\n",
       " 'throat',\n",
       " 'throw',\n",
       " 'thursday',\n",
       " 'ti',\n",
       " 'ticket',\n",
       " 'tie',\n",
       " 'tiffani',\n",
       " 'till',\n",
       " 'time',\n",
       " 'tini',\n",
       " 'tire',\n",
       " 'titan',\n",
       " 'titu',\n",
       " 'today',\n",
       " 'togeth',\n",
       " 'told',\n",
       " 'tom',\n",
       " 'tome',\n",
       " 'tomkat',\n",
       " 'tomorrow',\n",
       " 'ton',\n",
       " 'tonight',\n",
       " 'took',\n",
       " 'total',\n",
       " 'track',\n",
       " 'tragic',\n",
       " 'trailer',\n",
       " 'transamerica',\n",
       " 'travel',\n",
       " 'trece',\n",
       " 'tree',\n",
       " 'tri',\n",
       " 'trip',\n",
       " 'trivia',\n",
       " 'trouser',\n",
       " 'true',\n",
       " 'truli',\n",
       " 'truth',\n",
       " 'tun',\n",
       " 'turn',\n",
       " 'turner',\n",
       " 'tv',\n",
       " 'twice',\n",
       " 'twilight',\n",
       " 'twist',\n",
       " 'tye',\n",
       " 'type',\n",
       " 'uh',\n",
       " 'ultim',\n",
       " 'ultimatli',\n",
       " 'um',\n",
       " 'unabl',\n",
       " 'unauthor',\n",
       " 'unbeliev',\n",
       " 'undercov',\n",
       " 'understand',\n",
       " 'undoubtedli',\n",
       " 'unexpect',\n",
       " 'unfortun',\n",
       " 'unless',\n",
       " 'unpredict',\n",
       " 'updat',\n",
       " 'ur',\n",
       " 'url',\n",
       " 'usag',\n",
       " 'use',\n",
       " 'useless',\n",
       " 'usual',\n",
       " 'vacat',\n",
       " 'val',\n",
       " 'vampir',\n",
       " 'van',\n",
       " 'variou',\n",
       " 'vault',\n",
       " 've',\n",
       " 'veil',\n",
       " 'veri',\n",
       " 'versa',\n",
       " 'version',\n",
       " 'vic',\n",
       " 'vice',\n",
       " 'view',\n",
       " 'vigor',\n",
       " 'villain',\n",
       " 'vintag',\n",
       " 'virgin',\n",
       " 'visual',\n",
       " 'vito',\n",
       " 'vote',\n",
       " 'vs',\n",
       " 'wa',\n",
       " 'waaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'wack',\n",
       " 'wait',\n",
       " 'wal',\n",
       " 'walk',\n",
       " 'wanna',\n",
       " 'want',\n",
       " 'war',\n",
       " 'warn',\n",
       " 'wasn',\n",
       " 'watch',\n",
       " 'watson',\n",
       " 'way',\n",
       " 'weeeellllllll',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weiner',\n",
       " 'weird',\n",
       " 'went',\n",
       " 'wept',\n",
       " 'wesley',\n",
       " 'west',\n",
       " 'whatev',\n",
       " 'whenev',\n",
       " 'wherea',\n",
       " 'wherev',\n",
       " 'whi',\n",
       " 'whimper',\n",
       " 'whini',\n",
       " 'whistl',\n",
       " 'white',\n",
       " 'wholesom',\n",
       " 'wicca',\n",
       " 'wiccan',\n",
       " 'wick',\n",
       " 'wide',\n",
       " 'wif',\n",
       " 'win',\n",
       " 'winter',\n",
       " 'wish',\n",
       " 'witchcraft',\n",
       " 'witha',\n",
       " 'won',\n",
       " 'wonder',\n",
       " 'woo',\n",
       " 'word',\n",
       " 'work',\n",
       " 'world',\n",
       " 'wors',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'worthless',\n",
       " 'wotshisfac',\n",
       " 'wow',\n",
       " 'wrangler',\n",
       " 'write',\n",
       " 'writer',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'wussi',\n",
       " 'x3',\n",
       " 'xd',\n",
       " 'ya',\n",
       " 'yahoo',\n",
       " 'ye',\n",
       " 'yea',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'yesterday',\n",
       " 'yip',\n",
       " 'young',\n",
       " 'younger',\n",
       " 'yuck',\n",
       " 'yuh',\n",
       " 'zach',\n",
       " 'zen']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF after Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Z003RJMK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# library for regular expressions\n",
    "import re\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_tokens( doc ):\n",
    "    # Tokenize the documents to words\n",
    "    all_tokens = [word for word in nltk.word_tokenize(doc)]\n",
    "    clean_tokens = []\n",
    "    # remove the all characters other than alphabets. It takes a regex for matching.\n",
    "    for each_token in all_tokens:\n",
    "        if re.search('[a-zA-Z]', each_token):\n",
    "            clean_tokens.append(each_token)\n",
    "    \n",
    "    \n",
    "    # Stem the words\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=500,\n",
    "stop_words='english',\n",
    "tokenizer=get_stemmed_tokens,\n",
    "ngram_range=(1,2))\n",
    "feature_vector = tfidf_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = tfidf_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "train_X, test_X, train_y, test_y = train_test_split( train_ds_features,train_ds.sentiment,test_size = 0.3,random_state = 42 )\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit( train_X.toarray(), train_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       873\n",
      "           1       0.98      0.99      0.98      1203\n",
      "\n",
      "    accuracy                           0.98      2076\n",
      "   macro avg       0.98      0.98      0.98      2076\n",
      "weighted avg       0.98      0.98      0.98      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_ds_predicted = nb_clf.predict( test_X.toarray() )\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model using n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=500,stop_words='english',tokenizer=get_stemmed_tokens,ngram_range=(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = tfidf_vectorizer.fit( train_ds.text )\n",
    "train_ds_features = tfidf_vectorizer.transform( train_ds.text )\n",
    "features = feature_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'m\",\n",
       " \"'re\",\n",
       " \"'re gon\",\n",
       " \"'s\",\n",
       " \"'s great\",\n",
       " \"'s like\",\n",
       " \"'s mom\",\n",
       " \"'s onli\",\n",
       " \"'s retart\",\n",
       " \"'s right\",\n",
       " \"'s stupid\",\n",
       " \"'yeah\",\n",
       " \"'yeah got\",\n",
       " 'absolut',\n",
       " 'absolut awesom',\n",
       " 'accept',\n",
       " 'ach',\n",
       " 'ach cock',\n",
       " 'acn',\n",
       " 'acn love',\n",
       " 'alway',\n",
       " 'alway know',\n",
       " 'anyon',\n",
       " 'anyon say',\n",
       " 'ass',\n",
       " 'award',\n",
       " 'award remind',\n",
       " 'awesom',\n",
       " 'awesom book',\n",
       " 'awesom ca',\n",
       " 'awesom movi',\n",
       " \"awesom n't\",\n",
       " 'awesom stori',\n",
       " 'awesome..',\n",
       " 'b',\n",
       " 'b suck',\n",
       " 'beauti',\n",
       " 'becaus',\n",
       " 'becaus awesom',\n",
       " 'becaus hate',\n",
       " 'becaus know',\n",
       " 'becaus like',\n",
       " 'becaus love',\n",
       " 'becaus outshin',\n",
       " 'becaus type',\n",
       " 'becom',\n",
       " 'becom accept',\n",
       " 'begin',\n",
       " 'better',\n",
       " 'better read',\n",
       " 'big',\n",
       " 'big time',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'black guy',\n",
       " 'blond',\n",
       " 'blond rock-hard',\n",
       " 'bobbypin',\n",
       " 'bobbypin insan',\n",
       " 'bonker',\n",
       " 'book',\n",
       " 'book catcher',\n",
       " 'bore',\n",
       " 'brokeback',\n",
       " 'brokeback mountain',\n",
       " 'bye..',\n",
       " 'ca',\n",
       " \"ca n't\",\n",
       " 'care',\n",
       " 'care anyon',\n",
       " 'catcher',\n",
       " 'catcher tye',\n",
       " 'charact',\n",
       " 'charact die',\n",
       " 'clean',\n",
       " 'clean tabl',\n",
       " 'cock',\n",
       " 'code',\n",
       " 'code awesom',\n",
       " 'code awesome..',\n",
       " 'code film',\n",
       " 'code left',\n",
       " 'code suck',\n",
       " 'code sucked..',\n",
       " 'code wa',\n",
       " 'combin',\n",
       " 'combin opinion',\n",
       " 'commun',\n",
       " 'commun like',\n",
       " 'cool',\n",
       " 'cool hat',\n",
       " 'count',\n",
       " 'count book',\n",
       " 'cowboy',\n",
       " 'cowboy jokes..',\n",
       " 'coz',\n",
       " 'coz wa',\n",
       " 'crazi',\n",
       " 'crazi hate',\n",
       " 'cruis',\n",
       " 'da',\n",
       " 'da vinci',\n",
       " 'dad',\n",
       " \"dad 's\",\n",
       " 'daniel',\n",
       " 'daniel wotshisfac',\n",
       " 'dash',\n",
       " 'dash like',\n",
       " 'deep',\n",
       " 'deep profound',\n",
       " 'depress',\n",
       " 'depress movi',\n",
       " 'desper',\n",
       " \"desper love'th\",\n",
       " 'despis',\n",
       " 'despis movi',\n",
       " 'did',\n",
       " 'die',\n",
       " 'differ',\n",
       " 'doe',\n",
       " 'doe harri',\n",
       " 'draco',\n",
       " 'draco malfoy',\n",
       " 'drag',\n",
       " 'drag draco',\n",
       " 'dudee',\n",
       " 'dudee love',\n",
       " 'enjoy',\n",
       " 'escapad',\n",
       " 'escapad mission',\n",
       " 'evil',\n",
       " 'excel',\n",
       " 'eyr',\n",
       " 'eyr virgin',\n",
       " 'felicia',\n",
       " \"felicia 's\",\n",
       " 'felicia grab',\n",
       " 'film',\n",
       " 'freakin',\n",
       " 'freakin mission',\n",
       " 'friday',\n",
       " 'friday hung',\n",
       " 'friend',\n",
       " 'friend like',\n",
       " 'fuck',\n",
       " 'fuck horrible..',\n",
       " 'fuck slap',\n",
       " 'fun',\n",
       " 'gari',\n",
       " 'gari gin',\n",
       " 'gay',\n",
       " 'gay stupid',\n",
       " 'gin',\n",
       " 'gin zen',\n",
       " 'goin',\n",
       " 'goin mission',\n",
       " 'gon',\n",
       " 'gon na',\n",
       " 'good',\n",
       " 'good start',\n",
       " 'got',\n",
       " 'got acn',\n",
       " 'grab',\n",
       " 'grab key',\n",
       " 'great',\n",
       " 'great homosexu',\n",
       " 'groan',\n",
       " 'groan blond',\n",
       " 'guy',\n",
       " 'guy crazi',\n",
       " 'harri',\n",
       " 'harri potter',\n",
       " 'harri potter..',\n",
       " 'hat',\n",
       " 'hat head',\n",
       " 'hate',\n",
       " 'hate brokeback',\n",
       " 'hate da',\n",
       " 'hate harri',\n",
       " 'head',\n",
       " 'head laugh',\n",
       " 'heard',\n",
       " 'heard da',\n",
       " 'hella',\n",
       " 'hella like',\n",
       " 'help',\n",
       " 'help bobbypin',\n",
       " 'hi',\n",
       " 'hi hip',\n",
       " 'hi throat',\n",
       " 'hill',\n",
       " 'hill turn',\n",
       " 'hip',\n",
       " 'hip suck',\n",
       " 'homosexu',\n",
       " 'homosexu becom',\n",
       " 'hoot',\n",
       " 'horribl',\n",
       " 'horribl movi',\n",
       " 'horrible..',\n",
       " 'hung',\n",
       " 'hung kelsi',\n",
       " 'iii',\n",
       " 'imposs',\n",
       " 'imposs awesom',\n",
       " 'imposs bitch',\n",
       " 'imposs hoot',\n",
       " 'imposs iii',\n",
       " 'imposs movi',\n",
       " 'imposs rock',\n",
       " 'imposs station',\n",
       " 'imposs suck',\n",
       " 'imposs tom',\n",
       " 'imposs wa',\n",
       " 'insan',\n",
       " 'insan cool',\n",
       " 'jane',\n",
       " 'jane eyr',\n",
       " 'join',\n",
       " 'join commun',\n",
       " 'jokes..',\n",
       " 'just',\n",
       " 'just doe',\n",
       " 'just let',\n",
       " 'just plain',\n",
       " 'kate',\n",
       " 'kate escapad',\n",
       " 'kelsi',\n",
       " 'kelsi went',\n",
       " 'key',\n",
       " 'key dash',\n",
       " 'kirsten',\n",
       " 'kirsten leah',\n",
       " 'know',\n",
       " \"know 's\",\n",
       " 'know love',\n",
       " 'know want',\n",
       " 'laugh',\n",
       " 'laugh stupid',\n",
       " 'leah',\n",
       " 'leah kate',\n",
       " 'left',\n",
       " 'left right',\n",
       " 'let',\n",
       " 'let know',\n",
       " 'like',\n",
       " \"like 'yeah\",\n",
       " 'like freakin',\n",
       " 'like goin',\n",
       " 'like harri',\n",
       " 'like main',\n",
       " 'like make',\n",
       " 'like mission',\n",
       " 'like realli',\n",
       " 'like thi',\n",
       " 'like thing',\n",
       " 'like watch',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'love brokeback',\n",
       " 'love da',\n",
       " 'love harri',\n",
       " 'love kirsten',\n",
       " 'love luv',\n",
       " 'love mission',\n",
       " 'love sentri',\n",
       " \"love'th\",\n",
       " \"love'th da\",\n",
       " 'lubb',\n",
       " 'lubb da',\n",
       " 'luv',\n",
       " 'luv lubb',\n",
       " 'main',\n",
       " 'main charact',\n",
       " 'make',\n",
       " 'make friend',\n",
       " 'make whimper',\n",
       " 'malfoy',\n",
       " 'malfoy s',\n",
       " 'man',\n",
       " 'man love',\n",
       " 'materi',\n",
       " 'materi movi',\n",
       " 'mission',\n",
       " 'mission imposs',\n",
       " 'mom',\n",
       " 'mom clean',\n",
       " 'mountain',\n",
       " 'mountain beauti',\n",
       " 'mountain cowboy',\n",
       " 'mountain depress',\n",
       " 'mountain fuck',\n",
       " 'mountain horribl',\n",
       " 'mountain suck',\n",
       " 'mountain terribl',\n",
       " 'mountain think',\n",
       " 'mountain wa',\n",
       " 'movi',\n",
       " 'movi award',\n",
       " 'movi becaus',\n",
       " 'movi brokeback',\n",
       " 'movi just',\n",
       " 'movi realli',\n",
       " 'movi seen..',\n",
       " 'mtv',\n",
       " 'mtv movi',\n",
       " \"n't\",\n",
       " \"n't care\",\n",
       " \"n't wait\",\n",
       " 'na',\n",
       " 'na like',\n",
       " 'need',\n",
       " 'need fuck',\n",
       " 'nois',\n",
       " 'nois pant',\n",
       " 'oh',\n",
       " 'oh brokeback',\n",
       " 'ok',\n",
       " 'ok brokeback',\n",
       " 'ok bye..',\n",
       " 'onli',\n",
       " 'onli black',\n",
       " 'onli star',\n",
       " 'opinion',\n",
       " 'opinion review',\n",
       " 'outshin',\n",
       " 'outshin better',\n",
       " 'pant',\n",
       " 'pant groan',\n",
       " 'past',\n",
       " 'past hi',\n",
       " 'peopl',\n",
       " 'peopl seriou',\n",
       " 'peopl worth',\n",
       " 'person',\n",
       " 'person like',\n",
       " 'place',\n",
       " 'place peopl',\n",
       " 'plain',\n",
       " 'plain stupid',\n",
       " 'potter',\n",
       " \"potter 's\",\n",
       " 'potter awesom',\n",
       " 'potter becaus',\n",
       " 'potter daniel',\n",
       " 'potter deep',\n",
       " 'potter drag',\n",
       " 'potter movi',\n",
       " 'potter realli',\n",
       " 'potter seri',\n",
       " 'potter suck',\n",
       " 'potter thought',\n",
       " 'potter..',\n",
       " 'potter.. hate',\n",
       " 'pretti',\n",
       " 'profound',\n",
       " 'profound love',\n",
       " 'quiz',\n",
       " 'quiz suck',\n",
       " 'read',\n",
       " 'read harri',\n",
       " 'read materi',\n",
       " 'realiti',\n",
       " 'realiti coz',\n",
       " 'realli',\n",
       " 'realli depress',\n",
       " 'realli like',\n",
       " 'realli suck',\n",
       " 'realli want',\n",
       " 'remind',\n",
       " 'remind despis',\n",
       " 'retart',\n",
       " 'retart gay',\n",
       " 'review',\n",
       " 'review gari',\n",
       " 'right',\n",
       " 'right b',\n",
       " 'right left',\n",
       " 'rock',\n",
       " 'rock-hard',\n",
       " 'rock-hard ach',\n",
       " 's',\n",
       " 's trouser',\n",
       " 'said',\n",
       " 'said silent',\n",
       " 'saw',\n",
       " 'saw da',\n",
       " 'say',\n",
       " 'say differ',\n",
       " 'seen..',\n",
       " 'sentri',\n",
       " 'sentri mission',\n",
       " 'seri',\n",
       " 'seri becaus',\n",
       " 'seri count',\n",
       " 'seriou',\n",
       " 'seriou fun',\n",
       " 'silent',\n",
       " 'silent hill',\n",
       " 'sit',\n",
       " 'sit watch',\n",
       " 'slap',\n",
       " 'snuck',\n",
       " 'snuck brokeback',\n",
       " 'soo',\n",
       " 'soo onli',\n",
       " 'stand',\n",
       " 'stand mission',\n",
       " 'star',\n",
       " 'start',\n",
       " 'start read',\n",
       " 'station',\n",
       " 'station bonker',\n",
       " 'stori',\n",
       " 'stori harri',\n",
       " 'stupid',\n",
       " \"stupid 's\",\n",
       " 'stupid begin',\n",
       " 'stupid brokeback',\n",
       " 'suck',\n",
       " 'suck ass',\n",
       " 'suck big',\n",
       " 'suck harri',\n",
       " 'suck hi',\n",
       " 'suck just',\n",
       " 'suck ok',\n",
       " 'suck soo',\n",
       " 'sucked..',\n",
       " 'suicid',\n",
       " 'suicid yeah',\n",
       " 'tabl',\n",
       " 'tabl felicia',\n",
       " 'talk',\n",
       " 'terribl',\n",
       " 'terribl movi',\n",
       " 'thi',\n",
       " 'thi good',\n",
       " 'thi quiz',\n",
       " 'thing',\n",
       " 'thing like',\n",
       " 'think',\n",
       " \"think 's\",\n",
       " 'think hate',\n",
       " 'thought',\n",
       " 'thought join',\n",
       " 'throat',\n",
       " 'throat vigor',\n",
       " 'time',\n",
       " 'tom',\n",
       " 'tom cruis',\n",
       " 'trouser',\n",
       " 'trouser past',\n",
       " 'turn',\n",
       " 'turn realiti',\n",
       " 'tye',\n",
       " 'tye jane',\n",
       " 'type',\n",
       " 'type person',\n",
       " 'veri',\n",
       " 'vigor',\n",
       " 'vigor make',\n",
       " 'vinci',\n",
       " 'vinci code',\n",
       " 'virgin',\n",
       " 'virgin suicid',\n",
       " 'wa',\n",
       " 'wa absolut',\n",
       " 'wa awesom',\n",
       " 'wa beauti',\n",
       " 'wa bore',\n",
       " 'wa excel',\n",
       " 'wa hella',\n",
       " 'wa realli',\n",
       " 'wa terribl',\n",
       " 'wait',\n",
       " 'wait read',\n",
       " 'want',\n",
       " 'want becaus',\n",
       " 'want desper',\n",
       " 'want guy',\n",
       " 'want place',\n",
       " 'watch',\n",
       " 'watch mission',\n",
       " 'watch mtv',\n",
       " 'way',\n",
       " 'way da',\n",
       " 'went',\n",
       " 'went saw',\n",
       " 'whi',\n",
       " 'whi love',\n",
       " 'whi said',\n",
       " 'whimper',\n",
       " 'whimper nois',\n",
       " 'worth',\n",
       " 'worth know',\n",
       " 'wotshisfac',\n",
       " 'wotshisfac need',\n",
       " 'yeah',\n",
       " 'zen',\n",
       " 'zen da']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       873\n",
      "           1       0.96      1.00      0.98      1203\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2076\n",
      "   macro avg       0.98      0.97      0.97      2076\n",
      "weighted avg       0.97      0.97      0.97      2076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split( train_ds_features,train_ds.sentiment,test_size = 0.3,random_state = 42 )\n",
    "nb_clf = BernoulliNB()\n",
    "nb_clf.fit( train_X.toarray(), train_y )\n",
    "test_ds_predicted = nb_clf.predict( test_X.toarray() )\n",
    "print( metrics.classification_report( test_y, test_ds_predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE ML MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nb_clf, open(\"Sentiment_Classifier_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVOKE THE MODEL BACK AND USE IT for PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(\"Sentiment_Classifier_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_predicted = loaded_model.predict( test_X.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
