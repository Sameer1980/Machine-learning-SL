{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop Streaming. ----> Python API \n",
    "#[Hadoop Streamimg acts like a bridge between your Python code and the Java based HDFS, and lets you seamlessly access Hadoop \n",
    "#Clusters and execute MapReduce tasks.]\n",
    "# Mapper in Python -- Python supports map and reduce operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a list of numbers you want to square.\n",
    "#square = [1,2,3,4,5,6]\n",
    "#Square function is written as follows:\n",
    "#def square(num):\n",
    "#               return (num * num)\n",
    "\n",
    "#You can square this list using the following code:\n",
    "#squared_nums - map(square,numbers)\n",
    "\n",
    "#Output would be :\n",
    "#[1,4,9,16,25,36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce written in Python:\n",
    "#Suppose you want to sum the squared numbers:\n",
    "#[1,4,9,16,25,36]\n",
    "\n",
    "#Use the Sum function to add two numbers\n",
    "#def sum(a,b):\n",
    "#      return (a+b) \n",
    "\n",
    "# You can now sum the numbers using the reduce function\n",
    "# import functools as f\n",
    "# sum squared=f.reduce(sum,a)\n",
    "\n",
    "# Output would be:\n",
    "#[91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Spark uses In-Memory(RAM/Cache) instead of Disk I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapreduce requires a large number of servers and CPUs. Apache Spark requires a large number of servers , CPUs and a large and efficent RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Spark Resilient Distributed Datasets (RDD)\n",
    "# 1) The main programing approach of Spark is RDD.\n",
    "#2) They are fault-tolerant collections of objects spread across a cluster that you can operate on in parallel.They can automatically recover from machine failure.\n",
    "#3)You can create an RDD either by copying the elements from an existing collection or by referencing a dataset stored externally.\n",
    "#4)RDD supports two types of operations: transformations and actions.\n",
    "   #Transformations use an existing dataset to create a new one.Example: Map, filter , join.\n",
    "   # Actions compute on the dataset and return the value to the driver program.\n",
    "     #Example : Reduce , count , collect , save.\n",
    "    \n",
    "# If available memory is insufficient then Spark writes data to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages of Spark (PySpark is the Python API to access Spark programming model and perform data analysis)\n",
    "\n",
    "#10 to 100 times faster than Hadoop MapReduce.\n",
    "# Simple data processing framework.\n",
    "# Interactive APIs for Python for faster application development.\n",
    "# Has multiple tools for complex analytics operation.\n",
    "#Can be easily integrated with existing Hadoop infrastructure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark : RDD Transformations and actions\n",
    "\n",
    "# Tranformations\n",
    "#map() ---- Returns RDD , formed by passing data element of the source.\n",
    "# filter () --- Returns RDD based on selection.\n",
    "# flatMap() --- Maps items present in the dataset and returns sequence.\n",
    "# reduceByKey() --- Returns key value pairs where values for which each key is aggregrated by value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action\n",
    "# Collect() --- Returns all elements of the dataset as an array.\n",
    "# count() --- Returns the number of elements present in the dataset.\n",
    "# first() --- Returns the first element in the dataset.\n",
    "#take(n) --- Returns the number of elements (n) as specified by the number in the parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext or SC is the entry point to Spark for the Spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Tools\n",
    "\n",
    "# Spark SQL - Used for querying the dataset stored as RDD on HDFS through integrated APIs in Python, Java and Scala.\n",
    "# Spark Streaming - Useful for data streaming process.\n",
    "# MLlib (machine learning) - Used for machine learning processes such as Supervised and Unsupervised learning.\n",
    "# GraphX (graph) - It can be used to process or generate graphs with RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
