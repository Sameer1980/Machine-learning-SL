{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Machine Learning Approach\n",
    "    #The machine learning approach starts with either a problem that you need to solve or a given dataset that you need to ananlyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Understand the problem/dataset.\n",
    "#2) Extract the features from the dataset.\n",
    "#3)Identify the problem type.\n",
    "#4) Choose the right model.\n",
    "#5) Train and test the models.\n",
    "#6) Strive for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn ---- Problem-Solution Considerations\n",
    "#While working with Scikit Learn dataset or loading your data to Scikit Learn, always consider these four points \n",
    "#mentioned here:\n",
    "#1) Create separate objects for features and response.\n",
    "#2) Ensure that features and response have only numeric values.\n",
    "#3) Features and response should be in the form of NumPy ndarray.\n",
    "#4) Since features and response would be in the form of arrays , they would have shapes and sizes.\n",
    "#5)Features are always marked as X, response is mapped as Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=mx +c --------> Simple Linear equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression in Scikit-Learn.\n",
    "# sklearn.linear_model.LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)\n",
    "#fit_intercept -----> Calculates the intercept for this model.\n",
    "# normalize ---- > Normalizes the regression variable before performing the regression operation.\n",
    "# copy_X ------> Copies the regression variable.\n",
    "# n_jobs -----> Number of jobs to use for the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning -- Load and explore dataset.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Scikit-Learn dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset=load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (boston_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "#print the features of the dataset\n",
    "print (boston_dataset['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store data into a dataframe \n",
    "df_boston=pd.DataFrame(boston_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set features as columns on the dataframe\n",
    "df_boston.columns=boston_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view first 5 observations\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "#print dataset matrix(observations and features matrix)\n",
    "print (boston_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# print dataset target or response shape\n",
    "print (boston_dataset.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "#view targets or response\n",
    "print (boston_dataset['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - Linear Regression demo\n",
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the boston dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset=load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe and store the data\n",
    "df_boston=pd.DataFrame(boston_dataset.data)\n",
    "df_boston.columns=boston_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append Price , Target as a new column to the dataset\n",
    "df_boston['Price']=boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  Price  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print top 5 observations\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign features on X axis\n",
    "X_features=boston_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign target on Y axis\n",
    "Y_target= boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Linear model - the estimator \n",
    "from sklearn.linear_model import LinearRegression\n",
    "lineReg=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit data into the estimator\n",
    "lineReg.fit(X_features,Y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimated intercept 36.46 \n"
     ]
    }
   ],
   "source": [
    "# print the intercept\n",
    "print ('the estimated intercept %.2f '%lineReg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the coefficient is 13\n"
     ]
    }
   ],
   "source": [
    "# print the coefficient\n",
    "print ('the coefficient is %d' %len(lineReg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model split the whole dataset into train and test datasets\n",
    "#from sklearn import cross_validation --- depreceted\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,Y_train , Y_test = train_test_split(X_features,Y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\USER\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    backports.functools_lru_cache-1.6.4|     pyhd3eb1b0_0           9 KB\n",
      "    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB\n",
      "    conda-4.10.3               |   py38haa95532_0         2.9 MB\n",
      "    conda-package-handling-1.7.3|   py38h8cc25b3_1         721 KB\n",
      "    xmltodict-0.12.0           |     pyhd3eb1b0_0          13 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.6 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  backports.functoo~                             1.6.1-py_0 --> 1.6.4-pyhd3eb1b0_0\n",
      "  conda                                4.9.2-py38haa95532_0 --> 4.10.3-py38haa95532_0\n",
      "  conda-package-han~                   1.7.2-py38h76e460a_0 --> 1.7.3-py38h8cc25b3_1\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  backports.tempfile                               1.0-py_1 --> 1.0-pyhd3eb1b0_1\n",
      "  xmltodict                                     0.12.0-py_0 --> 0.12.0-pyhd3eb1b0_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "backports.functools_ | 9 KB      |            |   0% \n",
      "backports.functools_ | 9 KB      | ########## | 100% \n",
      "backports.functools_ | 9 KB      | ########## | 100% \n",
      "\n",
      "conda-4.10.3         | 2.9 MB    |            |   0% \n",
      "conda-4.10.3         | 2.9 MB    |            |   1% \n",
      "conda-4.10.3         | 2.9 MB    | 2          |   2% \n",
      "conda-4.10.3         | 2.9 MB    | 4          |   5% \n",
      "conda-4.10.3         | 2.9 MB    | 7          |   7% \n",
      "conda-4.10.3         | 2.9 MB    | 9          |   9% \n",
      "conda-4.10.3         | 2.9 MB    | #          |  11% \n",
      "conda-4.10.3         | 2.9 MB    | #1         |  12% \n",
      "conda-4.10.3         | 2.9 MB    | #3         |  14% \n",
      "conda-4.10.3         | 2.9 MB    | #4         |  15% \n",
      "conda-4.10.3         | 2.9 MB    | #6         |  16% \n",
      "conda-4.10.3         | 2.9 MB    | #7         |  18% \n",
      "conda-4.10.3         | 2.9 MB    | #9         |  19% \n",
      "conda-4.10.3         | 2.9 MB    | ##         |  20% \n",
      "conda-4.10.3         | 2.9 MB    | ##1        |  21% \n",
      "conda-4.10.3         | 2.9 MB    | ##2        |  23% \n",
      "conda-4.10.3         | 2.9 MB    | ##3        |  24% \n",
      "conda-4.10.3         | 2.9 MB    | ##5        |  25% \n",
      "conda-4.10.3         | 2.9 MB    | ##6        |  27% \n",
      "conda-4.10.3         | 2.9 MB    | ##7        |  28% \n",
      "conda-4.10.3         | 2.9 MB    | ##8        |  29% \n",
      "conda-4.10.3         | 2.9 MB    | ##9        |  30% \n",
      "conda-4.10.3         | 2.9 MB    | ###1       |  31% \n",
      "conda-4.10.3         | 2.9 MB    | ###1       |  32% \n",
      "conda-4.10.3         | 2.9 MB    | ###2       |  32% \n",
      "conda-4.10.3         | 2.9 MB    | ###2       |  33% \n",
      "conda-4.10.3         | 2.9 MB    | ###3       |  33% \n",
      "conda-4.10.3         | 2.9 MB    | ###3       |  34% \n",
      "conda-4.10.3         | 2.9 MB    | ###4       |  34% \n",
      "conda-4.10.3         | 2.9 MB    | ###4       |  35% \n",
      "conda-4.10.3         | 2.9 MB    | ###5       |  35% \n",
      "conda-4.10.3         | 2.9 MB    | ###5       |  36% \n",
      "conda-4.10.3         | 2.9 MB    | ###6       |  36% \n",
      "conda-4.10.3         | 2.9 MB    | ###7       |  37% \n",
      "conda-4.10.3         | 2.9 MB    | ###7       |  38% \n",
      "conda-4.10.3         | 2.9 MB    | ###8       |  38% \n",
      "conda-4.10.3         | 2.9 MB    | ###8       |  39% \n",
      "conda-4.10.3         | 2.9 MB    | ###9       |  39% \n",
      "conda-4.10.3         | 2.9 MB    | ###9       |  40% \n",
      "conda-4.10.3         | 2.9 MB    | ####       |  40% \n",
      "conda-4.10.3         | 2.9 MB    | ####       |  41% \n",
      "conda-4.10.3         | 2.9 MB    | ####1      |  41% \n",
      "conda-4.10.3         | 2.9 MB    | ####1      |  42% \n",
      "conda-4.10.3         | 2.9 MB    | ####2      |  42% \n",
      "conda-4.10.3         | 2.9 MB    | ####3      |  43% \n",
      "conda-4.10.3         | 2.9 MB    | ####3      |  44% \n",
      "conda-4.10.3         | 2.9 MB    | ####4      |  44% \n",
      "conda-4.10.3         | 2.9 MB    | ####4      |  45% \n",
      "conda-4.10.3         | 2.9 MB    | ####5      |  45% \n",
      "conda-4.10.3         | 2.9 MB    | ####5      |  46% \n",
      "conda-4.10.3         | 2.9 MB    | ####6      |  46% \n",
      "conda-4.10.3         | 2.9 MB    | ####6      |  47% \n",
      "conda-4.10.3         | 2.9 MB    | ####7      |  47% \n",
      "conda-4.10.3         | 2.9 MB    | ####7      |  48% \n",
      "conda-4.10.3         | 2.9 MB    | ####8      |  48% \n",
      "conda-4.10.3         | 2.9 MB    | ####9      |  49% \n",
      "conda-4.10.3         | 2.9 MB    | ####9      |  50% \n",
      "conda-4.10.3         | 2.9 MB    | #####      |  50% \n",
      "conda-4.10.3         | 2.9 MB    | #####      |  51% \n",
      "conda-4.10.3         | 2.9 MB    | #####1     |  51% \n",
      "conda-4.10.3         | 2.9 MB    | #####1     |  52% \n",
      "conda-4.10.3         | 2.9 MB    | #####2     |  52% \n",
      "conda-4.10.3         | 2.9 MB    | #####2     |  53% \n",
      "conda-4.10.3         | 2.9 MB    | #####3     |  53% \n",
      "conda-4.10.3         | 2.9 MB    | #####3     |  54% \n",
      "conda-4.10.3         | 2.9 MB    | #####4     |  54% \n",
      "conda-4.10.3         | 2.9 MB    | #####5     |  55% \n",
      "conda-4.10.3         | 2.9 MB    | #####5     |  56% \n",
      "conda-4.10.3         | 2.9 MB    | #####6     |  56% \n",
      "conda-4.10.3         | 2.9 MB    | #####6     |  57% \n",
      "conda-4.10.3         | 2.9 MB    | #####7     |  57% \n",
      "conda-4.10.3         | 2.9 MB    | #####7     |  58% \n",
      "conda-4.10.3         | 2.9 MB    | #####8     |  58% \n",
      "conda-4.10.3         | 2.9 MB    | #####8     |  59% \n",
      "conda-4.10.3         | 2.9 MB    | #####9     |  59% \n",
      "conda-4.10.3         | 2.9 MB    | #####9     |  60% \n",
      "conda-4.10.3         | 2.9 MB    | ######     |  60% \n",
      "conda-4.10.3         | 2.9 MB    | ######     |  61% \n",
      "conda-4.10.3         | 2.9 MB    | ######1    |  62% \n",
      "conda-4.10.3         | 2.9 MB    | ######2    |  62% \n",
      "conda-4.10.3         | 2.9 MB    | ######2    |  63% \n",
      "conda-4.10.3         | 2.9 MB    | ######3    |  63% \n",
      "conda-4.10.3         | 2.9 MB    | ######3    |  64% \n",
      "conda-4.10.3         | 2.9 MB    | ######4    |  64% \n",
      "conda-4.10.3         | 2.9 MB    | ######4    |  65% \n",
      "conda-4.10.3         | 2.9 MB    | ######5    |  65% \n",
      "conda-4.10.3         | 2.9 MB    | ######5    |  66% \n",
      "conda-4.10.3         | 2.9 MB    | ######6    |  66% \n",
      "conda-4.10.3         | 2.9 MB    | ######6    |  67% \n",
      "conda-4.10.3         | 2.9 MB    | ######7    |  68% \n",
      "conda-4.10.3         | 2.9 MB    | ######8    |  68% \n",
      "conda-4.10.3         | 2.9 MB    | ######8    |  69% \n",
      "conda-4.10.3         | 2.9 MB    | ######9    |  69% \n",
      "conda-4.10.3         | 2.9 MB    | ######9    |  70% \n",
      "conda-4.10.3         | 2.9 MB    | #######    |  70% \n",
      "conda-4.10.3         | 2.9 MB    | #######    |  71% \n",
      "conda-4.10.3         | 2.9 MB    | #######1   |  71% \n",
      "conda-4.10.3         | 2.9 MB    | #######1   |  72% \n",
      "conda-4.10.3         | 2.9 MB    | #######2   |  72% \n",
      "conda-4.10.3         | 2.9 MB    | #######2   |  73% \n",
      "conda-4.10.3         | 2.9 MB    | #######3   |  74% \n",
      "conda-4.10.3         | 2.9 MB    | #######4   |  74% \n",
      "conda-4.10.3         | 2.9 MB    | #######4   |  75% \n",
      "conda-4.10.3         | 2.9 MB    | #######5   |  75% \n",
      "conda-4.10.3         | 2.9 MB    | #######5   |  76% \n",
      "conda-4.10.3         | 2.9 MB    | #######6   |  76% \n",
      "conda-4.10.3         | 2.9 MB    | #######6   |  77% \n",
      "conda-4.10.3         | 2.9 MB    | #######7   |  77% \n",
      "conda-4.10.3         | 2.9 MB    | #######7   |  78% \n",
      "conda-4.10.3         | 2.9 MB    | #######8   |  78% \n",
      "conda-4.10.3         | 2.9 MB    | #######8   |  79% \n",
      "conda-4.10.3         | 2.9 MB    | #######9   |  80% \n",
      "conda-4.10.3         | 2.9 MB    | ########   |  80% \n",
      "conda-4.10.3         | 2.9 MB    | ########   |  81% \n",
      "conda-4.10.3         | 2.9 MB    | ########1  |  81% \n",
      "conda-4.10.3         | 2.9 MB    | ########1  |  82% \n",
      "conda-4.10.3         | 2.9 MB    | ########2  |  82% \n",
      "conda-4.10.3         | 2.9 MB    | ########2  |  83% \n",
      "conda-4.10.3         | 2.9 MB    | ########3  |  83% \n",
      "conda-4.10.3         | 2.9 MB    | ########3  |  84% \n",
      "conda-4.10.3         | 2.9 MB    | ########4  |  84% \n",
      "conda-4.10.3         | 2.9 MB    | ########4  |  85% \n",
      "conda-4.10.3         | 2.9 MB    | ########5  |  86% \n",
      "conda-4.10.3         | 2.9 MB    | ########6  |  86% \n",
      "conda-4.10.3         | 2.9 MB    | ########6  |  87% \n",
      "conda-4.10.3         | 2.9 MB    | ########7  |  87% \n",
      "conda-4.10.3         | 2.9 MB    | ########7  |  88% \n",
      "conda-4.10.3         | 2.9 MB    | ########8  |  88% \n",
      "conda-4.10.3         | 2.9 MB    | ########8  |  89% \n",
      "conda-4.10.3         | 2.9 MB    | ########9  |  89% \n",
      "conda-4.10.3         | 2.9 MB    | ########9  |  90% \n",
      "conda-4.10.3         | 2.9 MB    | #########  |  90% \n",
      "conda-4.10.3         | 2.9 MB    | #########  |  91% \n",
      "conda-4.10.3         | 2.9 MB    | #########1 |  91% \n",
      "conda-4.10.3         | 2.9 MB    | #########2 |  92% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conda-4.10.3         | 2.9 MB    | #########2 |  93% \n",
      "conda-4.10.3         | 2.9 MB    | #########3 |  93% \n",
      "conda-4.10.3         | 2.9 MB    | #########3 |  94% \n",
      "conda-4.10.3         | 2.9 MB    | #########4 |  94% \n",
      "conda-4.10.3         | 2.9 MB    | #########4 |  95% \n",
      "conda-4.10.3         | 2.9 MB    | #########5 |  95% \n",
      "conda-4.10.3         | 2.9 MB    | #########5 |  96% \n",
      "conda-4.10.3         | 2.9 MB    | #########6 |  96% \n",
      "conda-4.10.3         | 2.9 MB    | #########6 |  97% \n",
      "conda-4.10.3         | 2.9 MB    | #########7 |  97% \n",
      "conda-4.10.3         | 2.9 MB    | #########8 |  98% \n",
      "conda-4.10.3         | 2.9 MB    | #########8 |  99% \n",
      "conda-4.10.3         | 2.9 MB    | #########9 |  99% \n",
      "conda-4.10.3         | 2.9 MB    | #########9 | 100% \n",
      "conda-4.10.3         | 2.9 MB    | ########## | 100% \n",
      "conda-4.10.3         | 2.9 MB    | ########## | 100% \n",
      "\n",
      "backports.tempfile-1 | 11 KB     |            |   0% \n",
      "backports.tempfile-1 | 11 KB     | ########## | 100% \n",
      "backports.tempfile-1 | 11 KB     | ########## | 100% \n",
      "\n",
      "xmltodict-0.12.0     | 13 KB     |            |   0% \n",
      "xmltodict-0.12.0     | 13 KB     | ########## | 100% \n",
      "xmltodict-0.12.0     | 13 KB     | ########## | 100% \n",
      "\n",
      "conda-package-handli | 721 KB    |            |   0% \n",
      "conda-package-handli | 721 KB    | 2          |   2% \n",
      "conda-package-handli | 721 KB    | 4          |   4% \n",
      "conda-package-handli | 721 KB    | 8          |   9% \n",
      "conda-package-handli | 721 KB    | #3         |  13% \n",
      "conda-package-handli | 721 KB    | #7         |  18% \n",
      "conda-package-handli | 721 KB    | ##8        |  29% \n",
      "conda-package-handli | 721 KB    | ###5       |  36% \n",
      "conda-package-handli | 721 KB    | ####4      |  44% \n",
      "conda-package-handli | 721 KB    | #####3     |  53% \n",
      "conda-package-handli | 721 KB    | ######8    |  69% \n",
      "conda-package-handli | 721 KB    | #######9   |  80% \n",
      "conda-package-handli | 721 KB    | #########7 |  98% \n",
      "conda-package-handli | 721 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "conda update conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "#Print the dataset shape\n",
    "print (boston_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13) (127, 13) (379,) (127,)\n"
     ]
    }
   ],
   "source": [
    "#Print shapes of the training and testing datasets\n",
    "print (X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the training sets into the model\n",
    "lineReg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE value is 41.02 \n"
     ]
    }
   ],
   "source": [
    "# The mean square error (MSE) or residual sum of squares\n",
    "print ('MSE value is %.2f '% np.mean((lineReg.predict(X_test)-Y_test) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance score is 0.62 \n"
     ]
    }
   ],
   "source": [
    "#Calculate Variance ---- Closer the value is to 1 higher the model's accuracy.\n",
    "print('Variance score is %.2f '% lineReg.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              # Supervised Learning Models : Logistic Regression\n",
    "    #Logistic regression is a generalization of the linear regression model , used for classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sklearn.linear_model.LogisticRegression(penalty=l2,fit_intercept=True,dual=False,C=1.0,n_jobs=1,warm_start=False)\n",
    "# Penalty-- Specifies the norm used in penalization .\n",
    "# dual=False -- Implemented only for L2 penalty\n",
    "#C =1.0 ---- Inverse of regularization\n",
    "#warm_start --- If true , reuse the solution of the previous call.\n",
    "#n_jobs=1 --- Number of jobs in parallel computation.\n",
    "# random_state = None --- Seed or the random state instance .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning Models : K Nearest Neighbors (K_NN) \n",
    "# K-nearest neighbors, or K-NN, is one of the simplest machine learning algorithms used for both classification and regression\n",
    "# problem types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn Load dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the dataset type\n",
    "type(iris_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "#view information using dataset built in method DESCR (describe)\n",
    "print(iris_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "#view features\n",
    "print (iris_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# view target (response)\n",
    "print (iris_dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "# find number of observations\n",
    "print (iris_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign features data to X axis\n",
    "X_feature=iris_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign target data to Y axis\n",
    "Y_target=iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# View the shape for both axes.\n",
    "print (X_feature.shape)\n",
    "print (Y_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first use KNN classifier method - import it from sklearn ---- to find out the nearest neighbours of the datapoint for a given value of K.\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the KNN estimator-- The object used to instantiate the class of a learning model is called an estimator.\n",
    "knn=KNeighborsClassifier(n_neighbors=1) # pass the K neighbor value as 1 so that estimator will look for the first nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=1)\n"
     ]
    }
   ],
   "source": [
    "# print the knn\n",
    "print (knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit data into KNN model (estimator)\n",
    "knn.fit(X_feature,Y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object with new values for prediction\n",
    "X_new=[[3,5,4,1],[5,3,4,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the outcome for the new observation using knn classifier \n",
    "knn.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the logistic regression estimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#logReg=LogisticRegression(random_state=0,multi_class='multinomial',penalty='none',solver='newton-cg')\n",
    "logReg=LogisticRegression(random_state=None,multi_class='ovr',solver='liblinear')\n",
    "#logReg=LogisticRegression(random_state='none',solver='newton-cg',multi_class='ovr',penalty='none',max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'multinomial', 'n_jobs': None, 'penalty': 'none', 'random_state': 0, 'solver': 'newton-cg', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Print the params\n",
    "params=logReg.get_params()\n",
    "print (params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='ovr', solver='liblinear')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit data into the logistic regression estimator\n",
    "logReg.fit(X_feature,Y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the outcome using the Logistic Regression estimator\n",
    "logReg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unsupervised Learning Models: K-Means Clustering\n",
    "# A cluster is a group of similar data points.\n",
    "#It is used:\n",
    "# To extract the structure of the data.\n",
    "# To identify groups in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means finds the best centroids by alternatively assigning random centroids to a dataset and selecting mean data points\n",
    "# from the resulting clusters to form new centroids . It  continues this process iteratively until the model is optimised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.cluster.KMeans(n_clusters=8,init='k-means++',n_init=10,max_iter=300,tol=0.0001,precompute_distances='auto',verbose=0,random_state=none,copy_x=True,n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "\n",
    "# import KMeans class from sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# import make_blobs dataset from sklearn.cluster\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 1, 0, 2, 1, 0, 0, 2, 2, 2,\n",
       "       2, 2, 2, 1, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 2, 2, 1, 0, 2, 0, 1, 1,\n",
       "       2, 0, 1, 2, 1, 0, 2, 0, 1, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 2, 2,\n",
       "       0, 1, 2, 0, 0, 1, 2, 0, 1, 2, 1, 0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 2, 2, 1, 2, 0, 0, 1, 2, 0, 2, 0, 0, 0, 2, 0, 0, 1, 2, 2, 2, 2,\n",
       "       2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 2, 2,\n",
       "       1, 0, 2, 1, 2, 1, 1, 2, 2, 2, 1, 0, 1, 1, 1, 2, 0, 0, 2, 0, 2, 2,\n",
       "       2, 1, 2, 0, 1, 2, 1, 2, 1, 1, 2, 2, 1, 0, 1, 0, 0, 0, 1, 1, 2, 1,\n",
       "       1, 0, 0, 2, 1, 1, 0, 2, 0, 1, 2, 1, 2, 1, 2, 2, 0, 2, 2, 2, 0, 2,\n",
       "       1, 0, 2, 2, 1, 0, 0, 1, 1, 0, 1, 1, 2, 0, 1, 2, 2, 0, 0, 2, 2, 1,\n",
       "       2, 0, 1, 1, 0, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 2, 2, 2, 0, 1, 2, 0, 2, 0, 2, 1, 0, 0, 2, 0, 1, 1, 1, 2, 1, 1,\n",
       "       0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 0, 0, 0,\n",
       "       1, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define number of samples\n",
    "n_samples=300\n",
    "#define random state value to initialize the center\n",
    "random_state=20\n",
    "# define the number of feature as 5 \n",
    "X,y=make_blobs(n_samples=n_samples,n_features=5,random_state=None)\n",
    "#define the number of cluster to be formed as 3 and in random state and fit features into the model\n",
    "predict_y=KMeans(n_clusters=3,random_state=random_state).fit_predict(X)\n",
    "#print the estimator prediction\n",
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unsupervised Learning Models: Dimensionality Reduction\n",
    "# It reduces a high-dimensional dataset into a dataset with fewer dimensions.This makes it easier and faster for the algorithm to analyse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Techniques used for dimensionality reduction:\n",
    "# 1) Drop data columns with missing values.\n",
    "# 2) Drop data columns with low variance.\n",
    "# 3) Drop data columns with high correlations.\n",
    "# 4) Apply statistical functions - (PCA)Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Learning Models : Principal Component Analysis (PCA)\n",
    "# It is a linear dimensionality reduction method which uses singular value decomposition of the data and keeps only the\n",
    "#most significant singular vectors to project the data to a lower dimensional space.\n",
    "\n",
    "#1) It is primarily used to compress or reduce the data.\n",
    "#2) PCA tries to capture the variance , which helps it pick up interesting features.\n",
    "#3) PCA is used to reduce dimensionality in the dataset and to build our feature vector.\n",
    "#4) Here , the principal axis in the feature space represents the direction of mazimum variance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.decomposition.PCA(n_components=None,copy=True,whiten=False)\n",
    "#sklearn -- class\n",
    "#n_components -- Number of components to keep\n",
    "# copy= True -- Overwrites the transform data after fitting them into the model\n",
    "# whiten=False ---- Removes data with low variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "#  import required library PCA\n",
    "from sklearn.decomposition import PCA \n",
    "# import the dataset\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sample and random state\n",
    "n_sample=20\n",
    "random_state=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset with 10 features (dimension)\n",
    "X,y=make_blobs(n_samples=n_sample,n_features=10,random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the shape of the dataset.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the PCA estimator with number of reduced components\n",
    "pca=PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67060479 0.31024151 0.0058942 ]\n"
     ]
    }
   ],
   "source": [
    "# fit the data into the PCA estimator\n",
    "pca.fit(X)\n",
    "print (pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16883296  0.21332105 -0.44409182  0.41640712  0.45829818  0.25090813\n",
      "  0.14881592 -0.2534976  -0.20074121 -0.39456403]\n"
     ]
    }
   ],
   "source": [
    "# print the first PCA component\n",
    "first_pca=pca.components_[0]\n",
    "print (first_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the fitted data using transform method.\n",
    "pca_reduced=pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the reduced shape (lower dimension)\n",
    "pca_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE \n",
    "#Pipeline is mainly used to combine multiple models or estimators .Its characteristics are as follows:\n",
    "#1) It simplifies the process where more than one model is required or used.\n",
    "#2) Once all the data is fit into the models or estimators , the predict method can be called.\n",
    "#3) All models in the pipeline must be transformers. The last model can either be a transformer or a classifier, regressor,\n",
    "# , or other such objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimators are known as 'Model instance'\n",
    "# import pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import Linear estimatorestimator=[('dim_reductioestimator=[('dim_reduction',PCA(),('linear_model',LinearRegression())]n',PCA(),('linear_model',LinearRegression())]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# import PCA estimator for dimensionality reduction\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the estimators together\n",
    "estimator=[('dim_reduction',PCA()),('linear_model',LinearRegression())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the chain of estimators in a pipeline object\n",
    "pipeline_estimator=Pipeline(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dim_reduction', PCA()), ('linear_model', LinearRegression())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the chain of estimators\n",
    "pipeline_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dim_reduction', PCA())"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first step\n",
    "pipeline_estimator.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linear_model', LinearRegression())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View second step\n",
    "pipeline_estimator.steps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dim_reduction', PCA()), ('linear_model', LinearRegression())]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View all the steps in pipeline\n",
    "pipeline_estimator.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Model Persistence\n",
    "  #  Save model for future use . No need to retrain your model every time you need them.\n",
    "# 1) It is possible to save a model by using Python's Pickle model.\n",
    " #2) Scikit-learn has a special replacement for Pickle called joblib.\n",
    "# 3) You can use joblib.dump and joblib.load methods.\n",
    "# 4) These are more efficient for Big Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required Libraries and dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View feature names of the dataset.\n",
    "iris_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View target of the dataset\n",
    "iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define features and target objects.\n",
    "X_feature=iris_dataset.data\n",
    "Y_target=iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object with new values for prediction.\n",
    "X_new=[[3,5,4,1],[5,3,4,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Logistic regression estimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#logreg=LogisticRegression(random_state=0,multi_class='multinomial',penalty='none',solver='newton-cg')\n",
    "logreg=LogisticRegression(random_state=None,multi_class='ovr',solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='ovr', solver='liblinear')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit data into the Logistic regression estimator.\n",
    "logreg.fit(X_feature,Y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the outcome using Logistic regression estimator.\n",
    "logreg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for model persistence\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"\\x80\\x04\\x95)\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x1esklearn.linear_model._logistic\\x94\\x8c\\x12LogisticRegression\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x07penalty\\x94\\x8c\\x02l2\\x94\\x8c\\x04dual\\x94\\x89\\x8c\\x03tol\\x94G?\\x1a6\\xe2\\xeb\\x1cC-\\x8c\\x01C\\x94G?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\rfit_intercept\\x94\\x88\\x8c\\x11intercept_scaling\\x94K\\x01\\x8c\\x0cclass_weight\\x94N\\x8c\\x0crandom_state\\x94N\\x8c\\x06solver\\x94\\x8c\\tliblinear\\x94\\x8c\\x08max_iter\\x94Kd\\x8c\\x0bmulti_class\\x94\\x8c\\x03ovr\\x94\\x8c\\x07verbose\\x94K\\x00\\x8c\\nwarm_start\\x94\\x89\\x8c\\x06n_jobs\\x94N\\x8c\\x08l1_ratio\\x94N\\x8c\\x0en_features_in_\\x94K\\x04\\x8c\\x08classes_\\x94\\x8c\\x15numpy.core.multiarray\\x94\\x8c\\x0c_reconstruct\\x94\\x93\\x94\\x8c\\x05numpy\\x94\\x8c\\x07ndarray\\x94\\x93\\x94K\\x00\\x85\\x94C\\x01b\\x94\\x87\\x94R\\x94(K\\x01K\\x03\\x85\\x94h\\x1c\\x8c\\x05dtype\\x94\\x93\\x94\\x8c\\x02i4\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89C\\x0c\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x94t\\x94b\\x8c\\x05coef_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x03K\\x04\\x86\\x94h%\\x8c\\x02f8\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03h)NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x88C`\\x1d ?T\\xff@\\xda?ea5nM\\\\\\xdb?xz\\xa2\\x86\\xfbQ\\xfb\\xbf\\x0fh|N5m\\xf7?<\\xfb$3:\\xcb\\xf9\\xbf\\xb3\\x99m\\xbff\\x8c\\xf8\\xbfy\\xc8\\x01\\x01\\x8c\\x14\\x02\\xc0q\\xc9\\xc4e\\x18m\\xe2?\\xccs\\x82\\xa2\\x8a\\xc4\\x03@a\\xf08\\xe4(V\\xf0\\xbf\\x95[}\\x85\\xaf\\x7f\\xf6\\xbf\\x04M#\\nfq\\x04@\\x94t\\x94b\\x8c\\nintercept_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x03\\x85\\x94h4\\x89C\\x18\\xb2~?\\xd6\\xf4\\xe8\\xd0?/\\xd4\\xfc'\\xb7\\x80\\xf1?\\x11\\xc3\\xdc\\xe0ro\\xf3\\xbf\\x94t\\x94b\\x8c\\x07n_iter_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x01\\x85\\x94h%\\x8c\\x02i4\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03h)NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89C\\x04\\x07\\x00\\x00\\x00\\x94t\\x94b\\x8c\\x10_sklearn_version\\x94\\x8c\\x060.23.2\\x94ub.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use dumps method to persist the model\n",
    "persist_model=pkl.dumps(logreg)\n",
    "persist_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regresfilename.pk1']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use joblib.dump to persist the model to a file.\n",
    "#from sklearn.externals import joblib\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "joblib.dump(logreg,'regresfilename.pk1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use joblib.Load to persist the model to a file\n",
    "# Create new estimator from the saved model\n",
    "new_logreg_estimator=joblib.load('regresfilename.pk1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='ovr', solver='liblinear')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the new estimator\n",
    "new_logreg_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate and use new estimator to predict\n",
    "new_logreg_estimator.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation : Metric Functions\n",
    "#You can use the \"Metrics\" function to evaluate the accuracy of your model's predictions.\n",
    "# Classification --> metrics.accuracy_score & metrics.average_precision_score\n",
    "# Clustering  ----> metrics.adjusted_rand_score\n",
    "#Regression  ------> metrics.mean_absolute_error , metrics.mean_squared_error, metrics.median_absolute_error\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
